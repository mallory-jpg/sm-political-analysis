{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "import java.sql._\n",
    "import org.apache.spark.sql.SparkSession\n",
    "import org.apache.spark.sql.functions._\n",
    "import org.apache.spark.sql.types._\n",
    "import org.apache.spark.sql.streaming.Trigger\n",
    "import com.typesafe.config._\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "val config = ConfigFactory.load(\"application.conf\").getConfig(\"com.ram.batch\")\n",
    "val sparkConfig = config.getConfig(\"spark\")\n",
    "val mysqlConfig = config.getConfig(\"mysql\")\n",
    "val appName = sparkConfig.getString(BatchConstants.GET_APP_NAME)\n",
    "println(appName) // prints my-app\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "// configure Spark Session\n",
    "val spark = SparkSession\n",
    ".builder()\n",
    ".appName(\"Twitter Streaming\")\n",
    ".master(\"local[*]\")\n",
    ".getOrCreate()\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "// Define data source\n",
    "// here, it's the Kafka broker: tweetStream.get_twitter_data()\n",
    "val data_stream = spark\n",
    ".readStream // constantly expanding dataframe\n",
    ".format(\"kafka\")\n",
    ".option(\"kafka.bootstrap.servers\", \"localhost:9092\")\n",
    ".option(\"subscribe\", \"data-tweets\")\n",
    ".option(\"startingOffsets\", \"latest\") // or earliest\n",
    ".load()\n",
    "\n",
    "// schema of incoming data:\n",
    "// schema = StructType(\n",
    "    // StructField(key, BinaryType, true),\n",
    "    // StructField(value, BinaryType, true),\n",
    "    // StructField(topic, StringType, true),\n",
    "    // StructField(partition, IntegerType, true),\n",
    "    // StructField(offset, LongType, true),\n",
    "    // StructField(timestamp, TimestampType, true),\n",
    "    // StructField(timestampType, IntegerType, true)\n",
    "    // )\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "// watermarking to mark late-arriving data\n",
    "var data_stream_transformed = data_stream\n",
    ".withWatermark(\"timestamp\", \"1 day\")\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "// incoming tweet schema\n",
    "val schema = StructType(Seq(\n",
    "    StructField(\"created_at\", StringType, true),\n",
    "    StructField(\"id\", StringType, true),\n",
    "    StructField(\"id_str\", StringType, true),\n",
    "    StructField(\"text\", StringType, true),\n",
    "    StructField(\"retweet_count\", StringType, true),\n",
    "    StructField(\"favorite_count\", StringType, true),\n",
    "    StructField(\"favorited\", StringType, true),\n",
    "    StructField(\"retweeted\", StringType, true),\n",
    "    StructField(\"lang\", StringType, true),\n",
    "    StructField(\"location\", StringType, true)\n",
    "))\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "// data transformations\n",
    "data_stream_transformed = data_stream_transformed\n",
    ".selectExpr(\"CAST(value AS STRING) as json\") // byte - -> string\n",
    ".select(from_json(col(\"json\"), schema=schema).as(\"tweet\")) // json string - -> table defined by schema\n",
    "  .selectExpr( // data type casts\n",
    "              \"tweet.created_at\",\n",
    "               \"cast (tweet.id as long)\",\n",
    "               \"tweet.id_str\",\n",
    "               \"tweet.text\",\n",
    "               \"cast (tweet.retweet_count as integer)\",\n",
    "               \"cast (tweet.favorite_count as integer)\",\n",
    "               \"cast(tweet.favorited as boolean)\",\n",
    "               \"cast(tweet.retweeted as boolean)\",\n",
    "               \"tweet.lang as language_code\"\n",
    "              )\n",
    "// result = json tweets formatted in df\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "// group tweets by language code & count number of likes per tweet author\n",
    "data_stream_transformed = data_stream_transformed\n",
    ".groupBy(\"language_code\")\n",
    ".agg(sum(\"favorite_count\"), count(\"id\"))\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "// define database urls\n",
    "val url = \"jdbc:postgresql://192.168.10.157:5432/pipeline\"\n",
    "val user = \"pipeline\"\n",
    "val pw = \"Ch4ng3Me!\"\n",
    "val jdbcWriter = new PostgreSqlSink(url, user, pw)\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "// create trigger for stream updates\n",
    "val writeData = data_stream_transformed\n",
    ".writeStream\n",
    ".option(\"checkpointLocation\", \"/Users/dbg/Desktop/checkpoint\") // checkpoint prevents loss of state during downtime\n",
    ".foreach(writer)\n",
    ".trigger(Trigger.ProcessingTime(\"10 seconds\"))\n",
    ".outputMode(\"update\")\n",
    ".start()\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "// define postgresql sink\n",
    "\n",
    "\n",
    "class PostgreSqlSink(url: String, user: String, pwd: String) extends org.apache.spark.sql.ForeachWriter[org.apache.spark.sql.Row] {\n",
    "    val driver = \"org.postgresql.Driver\"\n",
    "    var connection: java.sql.Connection = _\n",
    "    var statement: java.sql.PreparedStatement = _\n",
    "\n",
    "    def open(partitionId: Long, version: Long): Boolean = {\n",
    "        Class.forName(driver)\n",
    "        connection = java.sql.DriverManager.getConnection(url, user, pwd)\n",
    "        connection.setAutoCommit(false)\n",
    "        statement = connection.prepareStatement(v_sql)\n",
    "        true\n",
    "    }\n",
    "\n",
    "    // process each tweet(row)\n",
    "    // copied directly from: https: // sonra.io/spark/advanced-spark-structured-streaming-aggregations-joins-checkpointing /\n",
    "    def process(value: org.apache.spark.sql.Row): Unit = {\n",
    "        statement.executeUpdate(\n",
    "            \"DELETE FROM stream_tweets WHERE stream_tweets.language_code = '\" +\n",
    "            value(0) + \"' ; \"\n",
    "        )\n",
    "        statement.executeUpdate(\n",
    "            \"INSERT INTO stream_tweets VALUES('\" + value(0) + \"', \" + value(\n",
    "                1) + \", \" + value(2) + \", '\" + value(3) + \"');\"\n",
    "        )\n",
    "    }\n",
    "    def close(errorOrNull: Throwable): Unit = {\n",
    "        connection.commit()\n",
    "        connection.close\n",
    "    }\n",
    "}\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "print(\"Starting stream...\")\n",
    "writeData.awaitTermination(20) // trigger executes ever 20 seconds\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "// get status of stream\n",
    "println(writeData.status)\n",
    "println(writeData.exception)\n",
    "println(writeData.lastProgress)\n",
    "writeData.explain()\n"
   ],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}