{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# TikTok Political Analysis\n",
    "## ~Objectives\n",
    "### Problems & Questions\n",
    "__How can we better develop educational materials to meet kids where they are?__\n",
    "- is it worth it to spend money to advertise to youth for political campaigns - are they engaging with current events?\n",
    "- what are kids talking about & why? What does our education system tell them and not tell them\n",
    "\n",
    "### Goals\n",
    "- understanding how age/youth impacts political indoctrination\n",
    "- understanding social impacts of political events\n",
    "- to understand colloquial knowledge of political concepts\n",
    "\n",
    "## ~Scope\n",
    "- daily batch updates\n",
    "- parsed news events triggers TikTok & twitter queries \n",
    "- topic counts 3 days before event cumulatively added to event day & 3 days following event\n",
    "- see trend lines of engagement on Twitter & TikTok\n",
    "\n",
    "### Overview:\n",
    "- Use NewsAPI to find top news by day\n",
    "- Parse news story title & article into individual words/phrases\n",
    "- Count most important individual words & phrases\n",
    "- Use top 3 most important words & phrases to create rules for searching the Twitter API\n",
    "- Count number of tweets mentioning words & phrases filtered by rules\n",
    "- Use top 3 words & phrases to find similar tags on TikTok API\n",
    "- Count number of TikTok challenges/tags/captions with top words & phrases\n",
    "\n",
    "## ~Extras\n",
    "- age inference of users\n",
    "- sentiment analysis (TextBlob)"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "---"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 1. Install Dependencies & Import Modules\n",
    "- Newsapi-python: pip install newsapi-python\n",
    "- Tweepy (install without virtual environment): pip install tweepy\n",
    "- playwright: pip install playwright\n",
    "                playwright install\n",
    "- TikTokAPI (install without virtual environment): pip install PyTikTokAPI\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "import requests\n",
    "from datetime import date, timedelta\n",
    "from bs4 import BeautifulSoup\n",
    "import numpy as np\n",
    "import logging\n",
    "import configparser\n",
    "from timer import Timer\n",
    "from numpy import datetime64\n",
    "from datetime import date, datetime, timedelta\n",
    "from nltk import tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from operator import itemgetter\n",
    "import math\n",
    "import tweepy  # python package for accessing Tweet streaming API\n",
    "from tweepy import API\n",
    "from tweepy import Stream\n",
    "import urllib.parse\n",
    "import psycopg2  # alts: SQLalchemy - warning: not as simple\n",
    "from TikTokAPI import TikTokAPI\n",
    "from selenium import webdriver\n",
    "from psycopg2 import Error\n",
    "import re\n",
    "import sys\n",
    "import geocoder\n",
    "from helper_functions import *\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Configure using config.ini file"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "source": [
    "c = configparser.ConfigParser()\n",
    "c.read('config.ini')\n",
    "\n",
    "# config credentials\n",
    "host = c['database']['host']\n",
    "username = c['database']['user']\n",
    "password = c['database']['password']\n",
    "db = c['database']['database']\n",
    "\n",
    "news_api_key = c['newsAuth']['api_key']\n",
    "tiktok_sv_id = c['tiktokAuth']['s_v_web_id']\n",
    "tiktok_tt_id = c['tiktokAuth']['tt_webid']\n",
    "# twitter_api_key = c['twitterAuth']['api_key']\n",
    "\n",
    "access_token = c['twitterAuth']['access_token']\n",
    "access_token_secret = c['twitterAuth']['access_token_secret']\n",
    "consumer_key = c['twitterAuth']['consumer_key']\n",
    "consumer_secret = c['twitterAuth']['consumer_secret']\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "create Database class"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "source": [
    "class DataBase():\n",
    "    def __init__(self, host_name, user_name, user_password):\n",
    "        self.host_name = host_name\n",
    "        self.user_name = user_name\n",
    "        self.user_password = user_password\n",
    "\n",
    "    def create_server_connection(self):\n",
    "        self.connection = None\n",
    "        try:\n",
    "            self.connection = psycopg2.connect(\n",
    "                host=self.host_name,\n",
    "                user=self.user_name,\n",
    "                password=self.user_password\n",
    "            )\n",
    "            logging.info(\"Database connection successful\")\n",
    "        except Error as err:\n",
    "            logging.error(f\"Error: '{err}'\")\n",
    "\n",
    "        return self.connection\n",
    "\n",
    "\n",
    "    def create_database(self, connection, query):\n",
    "            self.connection = connection\n",
    "            cursor = connection.cursor()\n",
    "            try:\n",
    "                cursor.execute(query)\n",
    "                logging.info(\"Database created successfully\")\n",
    "            except Error as err:\n",
    "                logging.error(f\"Error: '{err}'\")\n",
    "\n",
    "\n",
    "    def create_db_connection(self, db_name):\n",
    "            self.db_name = db_name\n",
    "            self.connection = None\n",
    "            try:\n",
    "                self.connection = psycopg2.connect(\n",
    "                    host=self.host_name,\n",
    "                    user=self.user_name,\n",
    "                    password=self.user_password,\n",
    "                    database=self.db_name\n",
    "                )\n",
    "                # cursor = connection.cursor()\n",
    "                logging.info(\"PostgreSQL Database connection successful\")\n",
    "            except psycopg2.Error as err:\n",
    "                logging.error(f\"Error: '{err}'\")\n",
    "\n",
    "            return self.connection\n",
    "\n",
    "    # @Timer(name='Query Execution') #*TODO fix __enter__ attribute error\n",
    "    def execute_query(self, connection, query):\n",
    "            self.connection = connection\n",
    "            cursor = connection.cursor()\n",
    "            try:\n",
    "                cursor.execute(query)\n",
    "                self.connection.commit()\n",
    "                logging.info(\"Query successful\")\n",
    "            except Error as err:\n",
    "                print(f\"Error: '{err}'\")\n",
    "    \n",
    "    def read_query(self, connection, query):\n",
    "        self.connection = connection\n",
    "        cursor = self.connection.cursor()\n",
    "        result = None\n",
    "        try:\n",
    "            cursor.execute(query)\n",
    "            result = cursor.fetchall()\n",
    "            return result\n",
    "        except Error as err:\n",
    "            logging.error(f\"Error: '{err}'\")\n",
    "    \n",
    "\n",
    "    # @Timer(name='Mogrify')\n",
    "    def execute_mogrify(self, conn, df, table):\n",
    "        \"\"\"\n",
    "        Using cursor.mogrify() to build the bulk insert query\n",
    "        then cursor.execute() to execute the query\n",
    "        \"\"\"\n",
    "        self.connection = conn\n",
    "        # Create a list of tupples from the dataframe values\n",
    "        tuples = [tuple(x) for x in df.to_numpy()]\n",
    "    \n",
    "        # Comma-separated dataframe columns\n",
    "        cols = ','.join(list(df.columns))\n",
    "    \n",
    "        # SQL query to execute\n",
    "        cursor = conn.cursor()\n",
    "        values = [cursor.mogrify(\"(%s,%s,%s,%s)\", tup).decode('utf8')\n",
    "                for tup in tuples]\n",
    "        # if not publishedAt, delete record\n",
    "        query = \"INSERT INTO %s(%s) VALUES\" % (table, cols) + \",\".join(values)\n",
    "\n",
    "        try:\n",
    "            cursor.execute(query, tuples)\n",
    "            conn.commit()\n",
    "        except (Exception, psycopg2.DatabaseError) as error:\n",
    "            logging.error(\"Error: %s\" % error)\n",
    "            print(\"Error: %s\" % error)\n",
    "            conn.rollback()\n",
    "            cursor.close()\n",
    "            conn.close()\n",
    "            return 1\n",
    "        logging.info(\"execute_mogrify() done\")\n",
    "        cursor.close()\n",
    "        conn.close()\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Variables for SQL queries"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# create db \n",
    "create_database_query = \"\"\"\n",
    "        CREATE DATABASE IF NOT EXISTS sm_news;\n",
    "    \"\"\"\n",
    "# create necessary tables\n",
    "create_article_table = \"\"\"\n",
    "    CREATE TABLE IF NOT EXISTS articles (\n",
    "        publishedAt DATE,\n",
    "        title VARCHAR PRIMARY KEY,\n",
    "        author VARCHAR,\n",
    "        url TEXT\n",
    "        );\n",
    "    \"\"\"\n",
    "create_article_table_index = \"\"\"\n",
    "    CREATE INDEX index\n",
    "        ON articles(publishedAt,\n",
    "            title\n",
    "        );\n",
    "    \"\"\"\n",
    "create_article_text_table = \"\"\"\n",
    "    CREATE TABLE IF NOT EXISTS article_text (\n",
    "        title VARCHAR PRIMARY KEY,\n",
    "        article_text TEXT\n",
    "        );\n",
    "    \"\"\"\n",
    "create_political_event_table = \"\"\"\n",
    "    CREATE TABLE IF NOT EXISTS event (\n",
    "        eventID ID PRIMARY KEY,\n",
    "        startDate DATE,\n",
    "        name VARCHAR NOT NULL,\n",
    "        description VARCHAR NOT NULL,\n",
    "        keyWords VARCHAR\n",
    "        );\n",
    " \"\"\"\n",
    "create_tweets_table = \"\"\"\n",
    "    CREATE TABLE IF NOT EXISTS tweets (\n",
    "        tweet_id INT PRIMARY KEY,\n",
    "        publishedAt DATE NOT NULL,\n",
    "        userID VARCHAR NOT NULL,\n",
    "        tweet VARCHAR NOT NULL,\n",
    "        location VARCHAR NOT NULL, \n",
    "        tags VARCHAR NOT NULL\n",
    "        );\n",
    "    \"\"\"\n",
    "create_tiktoks_table = \"\"\"\n",
    "    CREATE TABLE IF NOT EXISTS tiktoks (\n",
    "        postID INT PRIMARY KEY,\n",
    "        createTime DATE NOT NULL,\n",
    "        userID INT NOT NULL,\n",
    "        description VARCHAR NOT NULL,\n",
    "        musicID INT NOT NULL,\n",
    "        soundID INT NOT NULL,\n",
    "        tags VARCHAR NOT NULL\n",
    "        );\n",
    "    \"\"\"\n",
    "create_tiktok_sounds_table = \"\"\"\n",
    "    CREATE TABLE IF NOT EXISTS tiktok_sounds (\n",
    "        soundID INT PRIMARY KEY,\n",
    "        soundTitle VARCHAR,\n",
    "        isOriginal BOOLEAN\n",
    "        );\n",
    "    \"\"\"\n",
    "create_tiktok_music_table = \"\"\"\n",
    "    CREATE TABLE IF NOT EXISTS tiktok_music (\n",
    "        songID INT PRIMARY KEY,\n",
    "        songTitle VARCHAR NOT NULL\n",
    "        );\n",
    "    \"\"\"\n",
    "\n",
    "create_tiktok_stats_table = \"\"\"\n",
    "    CREATE TABLE IF NOT EXISTS tiktok_stats (\n",
    "        postID INT PRIMARY KEY,\n",
    "        shareCount INT,\n",
    "        commentCount INT,\n",
    "        playCount INT,\n",
    "        diggCount INT\n",
    "        );\n",
    "    \"\"\"\n",
    "\n",
    "create_tiktok_tags_table = \"\"\"\n",
    "    CREATE TABLE IF NOT EXISTS tiktok_tags (\n",
    "        tagID INT PRIMARY KEY,\n",
    "        tag_name VARCHAR NOT NULL \n",
    "        );\n",
    "    \"\"\"\n",
    "create_users_table = \"\"\"\n",
    "    CREATE TABLE IF NOT EXISTS users (\n",
    "        userID INT PRIMARY KEY,\n",
    "        username VARCHAR NOT NULL,\n",
    "        user_bio VARCHAR NOT NULL\n",
    "        );\n",
    "    \"\"\"\n",
    "delete_bad_data = \"\"\"\n",
    "    DELETE FROM articles\n",
    "        WHERE publishedAt IS NULL;\n",
    "    \"\"\"\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Create Database"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "source": [
    "postgres_db = DataBase(host, username, password)\n",
    "\n",
    "# connect to server\n",
    "postgres_server = postgres_db.create_server_connection()\n",
    "\n",
    "# connect to social media news db\n",
    "connection = postgres_db.create_db_connection(db)\n",
    "\n",
    "\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# execute defined queries to create db tables if needed\n",
    "try:\n",
    "    # TODO fix attribute error __enter__ for Timer wrapper\n",
    "    postgres_db.execute_query(connection, create_article_table)\n",
    "    postgres_db.execute_query(connection, create_article_text_table)\n",
    "    postgres_db.execute_query(connection, create_tweets_table)\n",
    "    postgres_db.execute_query(connection, create_political_event_table)\n",
    "    postgres_db.execute_query(connection, create_users_table)\n",
    "    postgres_db.execute_query(connection, create_tiktok_sounds_table)\n",
    "    postgres_db.execute_query(connection, create_tiktok_music_table)\n",
    "    postgres_db.execute_query(connection, create_tiktok_stats_table)\n",
    "    postgres_db.execute_query(connection, create_tiktok_tags_table)\n",
    "    postgres_db.execute_query(connection, create_tiktoks_table)\n",
    "except (ConnectionError) as e:\n",
    "    logging.error({e}, 'Check SQL create queries')\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# add foreign keys\n",
    "alter_tiktoks_table = \"\"\"\n",
    "    ALTER TABLE tiktoks\n",
    "    ADD FOREIGN KEY(musicID) REFERENCES tiktok_music(songID),\n",
    "    ADD FOREIGN KEY(soundID) REFERENCES tiktok_sounds(soundID),\n",
    "    ADD FOREIGN KEY(userID) REFERENCES users(userID)\n",
    "    ON DELETE SET NULL;\n",
    "\"\"\"\n",
    "alter_tiktok_stats_table = \"\"\"\n",
    "    ALTER TABLE tiktok_stats\n",
    "    ADD FOREIGN KEY(postID) REFERENCES tiktoks(postID)\n",
    "    ON DELETE SET NULL;\n",
    "\"\"\"\n",
    "try:\n",
    "    postgres_db.execute_query(connection, alter_tiktoks_table)\n",
    "    postgres_db.execute_query(connection, alter_tiktok_stats_table)\n",
    "except (ConnectionError) as e:\n",
    "    logging.error({e}, 'Check SQL alteration queries. Are the foreign key restraints valid?')\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 2. Find Top News by Day"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Create News class"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "source": [
    "class News():\n",
    "    \"\"\"Extract keywords from  news articles to use as search values for TikTok & Twitter posts relating to the political event of interest. \"\"\"\n",
    "\n",
    "    def __init__(self, api_key, logger=logging):\n",
    "        self.api_key = api_key\n",
    "        self.logger = logging.basicConfig(filename='news.log', filemode='w',\n",
    "                    format=f'%(asctime)s - %(levelname)s - %(message)s')\n",
    "\n",
    "    def request_pop_news(self, params={\n",
    "        'q': ['politics' or 'political' or 'law' or 'legal' or 'policy'],\n",
    "        'from': {date.today() - timedelta(days=3)},\n",
    "        'to': {date.today},\n",
    "        'language': 'en',\n",
    "        'sort_by': 'popularity'\n",
    "    }):\n",
    "        pop_news = []\n",
    "        self.params = params\n",
    "\n",
    "        headers = {\n",
    "            'X-Api-Key': self.api_key,\n",
    "            # get_random_ua for Chrome\n",
    "            'user-agent': get_random_ua('Chrome')\n",
    "        }\n",
    "\n",
    "        url = 'https://newsapi.org/v2/everything'\n",
    "\n",
    "        try:\n",
    "            # response as JSON dict\n",
    "            self.response = requests.get(\n",
    "                url, params=self.params, headers=headers).json()  # backoff_factor=1, verify=False\n",
    "        except requests.ConnectionError as error:\n",
    "            logging.error(f'Connection error: {error}\\n Likely too many requests. Try using a proxy or changing user agents.')\n",
    "        else:\n",
    "            with open('pop_news.json', 'w') as f:\n",
    "                # write results to JSON file\n",
    "                json.dump(self.response, f)\n",
    "\n",
    "            with open('pop_news.json', 'r') as file:\n",
    "                # create Python list object from JSON\n",
    "                pop_news_json = file.read().split(\"\\n\")\n",
    "\n",
    "                for story in pop_news_json:\n",
    "                    pop_obj = json.loads(story)\n",
    "\n",
    "                    if 'title' in pop_obj:\n",
    "                        pop_obj['title'] = pop_obj['articles']['title']\n",
    "                    if 'author' in pop_obj:\n",
    "                        pop_obj['author'] = pop_obj['articles']['author']\n",
    "                    if 'url' in pop_obj:\n",
    "                        pop_obj['url'] = pop_obj['articles']['url']\n",
    "                    if 'publishedAt' in pop_obj:\n",
    "                        pop_obj['publishedAt'] = pop_obj['articles']['publishedAt']\n",
    "\n",
    "                    # add info to pop_news dict\n",
    "                    pop_news.append(pop_obj)\n",
    "                    # self.news_counter += 1\n",
    "            logging.info('Pop news request successful')\n",
    "        \n",
    "        return pop_news\n",
    "\n",
    "\n",
    "    def get_top_headlines(self, params={\n",
    "        \"language\": \"en\",\n",
    "        \"country\": \"us\"\n",
    "    }):\n",
    "        top_headlines = []\n",
    "        self.params = params\n",
    "        # self.news_counter = 0\n",
    "\n",
    "        headers = {\n",
    "            \"X-Api-Key\": news_api_key,\n",
    "            \"user-agent\": get_random_ua('Chrome')\n",
    "            }\n",
    "        url = \"https://newsapi.org/v2/top-headlines\"\n",
    "\n",
    "        try:  # backoff_factor = 1 so successive sleep between failed requests are 0.5, 1, 2, 4, 8, 16, 32, 64, 128, 256\n",
    "            self.response = requests.get(\n",
    "                    url, params=self.params, headers=headers).json()  # response JSON dict\n",
    "        except requests.exceptions.ConnectionError:\n",
    "            self.response.status_code = \"Connection refused\"\n",
    "                # break\n",
    "        else:\n",
    "            with open(\"top_headlines.json\", \"w\") as f:\n",
    "                    # write results to JSON file\n",
    "                json.dump(self.response, f)\n",
    "\n",
    "            with open(\"top_headlines.json\", \"r\") as file:\n",
    "                    # create Python object from JSON\n",
    "                    top_headlines_json = file.read().split(\"\\n\")\n",
    "\n",
    "                    for story in top_headlines_json:\n",
    "                        story_obj = json.loads(story)\n",
    "\n",
    "                        if 'title' in story_obj:\n",
    "                            story_obj[\"title\"] = story_obj[\"articles\"][\"title\"]\n",
    "                        if 'author' in story_obj:\n",
    "                            story_obj[\"author\"] = story_obj[\"articles\"][\"author\"]\n",
    "                        if 'url' in story_obj:\n",
    "                            story_obj[\"url\"] = story_obj[\"articles\"][\"url\"]\n",
    "                        if 'publishedAt' in story_obj:\n",
    "                            story_obj[\"publishedAt\"] = story_obj[\"articles\"][\"publishedAt\"]\n",
    "\n",
    "                        # add info to top_headlines list/dict\n",
    "                        top_headlines.append(story_obj)\n",
    "                        # self.news_counter += 1\n",
    "            logging.info('News top headlines request successful')\n",
    "        \n",
    "            return top_headlines\n",
    "\n",
    "\n",
    "    def get_all_news(self):\n",
    "        \"\"\"Combines top headlines and popular news into one Pandas DataFrame.\"\"\"\n",
    "\n",
    "        # call .get_top_headlines() and .request_pop_news()\n",
    "        top_headlines = self.get_top_headlines()\n",
    "        pop_news = self.request_pop_news()\n",
    "        logging.info('News requests successful')\n",
    "\n",
    "        # noramlize nested JSON results\n",
    "        pop_news = pd.json_normalize(pop_news, record_path=['articles'])\n",
    "        top_headlines = pd.json_normalize(\n",
    "            top_headlines, record_path=['articles'])\n",
    "        all_news = top_headlines.append(pop_news)\n",
    "\n",
    "        # create dataframe from combined news list\n",
    "        self.all_news_df = pd.DataFrame(\n",
    "                all_news, columns=['title', 'author', 'url', 'publishedAt', \"text\"])\n",
    "        self.all_news_df.drop_duplicates()\n",
    "\n",
    "        # convert to datetime\n",
    "        self.all_news_df['publishedAt'] = self.all_news_df['publishedAt'].map(\n",
    "                lambda row: datetime.strptime(str(row), \"%Y-%m-%dT%H:%M:%SZ\") if pd.notnull(row) else row)\n",
    "\n",
    "        # set index to publishing time, inplace to apply to same df instead of copy or view\n",
    "        self.all_news_df.set_index('publishedAt', inplace=True)\n",
    "\n",
    "        # apply .get_article_text() to text column of df\n",
    "        self.all_news_df[\"text\"] = self.all_news_df[\"url\"].apply(\n",
    "                self.get_article_text)\n",
    "\n",
    "        # get keywords from article text\n",
    "        self.all_news_df[\"keywords\"] = self.all_news_df['text'].apply(\n",
    "                self.keyword_extraction)\n",
    "        # get top n=3 words of significance\n",
    "        self.all_news_df[\"keywords\"] = self.all_news_df[\"keywords\"].apply(\n",
    "                self.get_top_n, n=3)\n",
    "\n",
    "        # print(len(all_news))\n",
    "        logging.info(f'Stored {len(all_news)} news stories in dataframe')\n",
    "        \n",
    "        return self.all_news_df\n",
    "\n",
    "    \n",
    "    def get_article_text(self, url):\n",
    "        \"\"\"Clean & process news article text to prepare for keyword extraction\"\"\"\n",
    "\n",
    "        contractions_dict = {\"'s\": \" is\", \"n't\": \" not\", \"'m\": \" am\", \"'ll\": \" will\",\n",
    "                             \"'d\": \" would\", \"'ve\": \" have\", \"'re\": \" are\"}\n",
    "        symbols_list = ['&', '+', '-', '/', '|', '$', '%', ':', '(', ')', '?', \"'\", ';', ',']\n",
    "\n",
    "        try:\n",
    "            # request\n",
    "            r = requests.get(url)\n",
    "            html = r.text\n",
    "            soup = BeautifulSoup(html)\n",
    "            a_text = soup.get_text()\n",
    "        except requests.RequestException as ex:\n",
    "            logging.exception(ex, 'Issue with article text requests')\n",
    "        else:\n",
    "            # remove newline characters\n",
    "            a_text = a_text.strip()\n",
    "            # split joined words\n",
    "            a_text = \" \".join([s for s in re.split(\n",
    "                \"([A-Z][a-z]+[^A-Z]*)\", a_text) if s])\n",
    "            # remove mentions\n",
    "            a_text = re.sub(\"@\\S+\", \" \", a_text)\n",
    "            # remove URLs\n",
    "            a_text = re.sub(\"https*\\S+\", \" \", a_text)\n",
    "            # remove hashtags\n",
    "            a_text = re.sub(\"#\\S+\", \" \", a_text)\n",
    "            # remove unicode characters\n",
    "            a_text = a_text.encode('ascii', 'ignore').decode()\n",
    "            # replace contractions\n",
    "            for key, value in contractions_dict.items():\n",
    "                if key in a_text:\n",
    "                    a_text = a_text.replace(key, value)\n",
    "            # remove symbols and punctuation\n",
    "            for i in symbols_list:\n",
    "                if i in a_text:\n",
    "                    a_text = a_text.replace(i, '')\n",
    "\n",
    "            # make lowercase\n",
    "            a_text = a_text.lower()\n",
    "            a_text = re.sub(r'\\w*\\d+\\w*', '', a_text)\n",
    "        \n",
    "        return a_text\n",
    "\n",
    "\n",
    "    def keyword_extraction(self, text):\n",
    "        \"\"\"Determine weight of important words in articles and add to articles_text table\n",
    "        using TF-IDF ranking\"\"\"\n",
    "\n",
    "        # make sure text is in string format for parsing\n",
    "        text = str(text)\n",
    "        stop_words = set(stopwords.words('english'))\n",
    "\n",
    "        # find total words in document for calculating Term Frequency (TF)\n",
    "        total_words = text.split()\n",
    "        total_word_length = len(total_words)\n",
    "\n",
    "        # find total number of sentences for calculating Inverse Document Frequency\n",
    "        total_sentences = tokenize.sent_tokenize(text)\n",
    "        total_sent_len = len(total_sentences)\n",
    "\n",
    "        # calculate TF for each word\n",
    "        tf_score = {}\n",
    "        for each_word in total_words:\n",
    "            each_word = each_word.replace('.', '')\n",
    "            if each_word not in stop_words and len(each_word) > 3:\n",
    "                if each_word in tf_score:\n",
    "                    tf_score[each_word] += 1\n",
    "                else:\n",
    "                    tf_score[each_word] = 1\n",
    "\n",
    "        # Divide by total_word_length for each dictionary element\n",
    "        tf_score.update((x, y/int(total_word_length))\n",
    "                        for x, y in tf_score.items())  # TODO test - ZeroError\n",
    "\n",
    "        #calculate IDF for each word\n",
    "        idf_score = {}\n",
    "        for each_word in total_words:\n",
    "            each_word = each_word.replace('.', '')\n",
    "            if each_word not in stop_words and len(each_word) > 3:\n",
    "                if each_word in idf_score:\n",
    "                    idf_score[each_word] = self.check_sent(each_word, total_sentences)\n",
    "                else:\n",
    "                    idf_score[each_word] = 1\n",
    "\n",
    "        # Performing a log and divide\n",
    "        idf_score.update((x, math.log(int(total_sent_len)/y))\n",
    "                        for x, y in idf_score.items())\n",
    "\n",
    "        # Calculate IDF * TF for each word\n",
    "        tf_idf_score = {key: tf_score[key] *\n",
    "                        idf_score.get(key, 0) for key in tf_score.keys()}\n",
    "\n",
    "        return tf_idf_score\n",
    "\n",
    "    def check_sent(self, word, sentences):\n",
    "        \"\"\"Check if word is present in sentence list for calculating IDF (Inverse Document Frequency)\"\"\"\n",
    "        final = [all([w in x for w in word]) for x in sentences]\n",
    "        sent_len = [sentences[i] for i in range(0, len(final)) if final[i]]\n",
    "    \n",
    "        return int(len(sent_len))\n",
    "\n",
    "    def get_top_n(self, dict_elem, n):\n",
    "        \"\"\"Calculate most important keywords in text of interest\"\"\"\n",
    "        result = dict(sorted(dict_elem.items(),\n",
    "                     key=itemgetter(1), reverse=True)[:n])\n",
    "        result = result.keys()\n",
    "\n",
    "        return result\n",
    "\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 3. Parse Titles & Articles"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "source": [
    "# instantiate News class\n",
    "news = News(news_api_key)\n",
    "# get all news - takes about 30 seconds\n",
    "news.get_all_news()"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "                                                                 title  \\\n",
       "publishedAt                                                              \n",
       "2021-09-01 21:57:13  Joe Rogan, A Podcasting Giant Who Has Been Dis...   \n",
       "2021-09-01 21:13:20  Social Security costs to exceed revenue for 1s...   \n",
       "2021-09-01 20:50:36  Syed Ali Geelani: Kashmir's separatist patriar...   \n",
       "2021-09-01 20:42:53  Purdue Pharma Is Dissolved and Sacklers Pay $4...   \n",
       "2021-09-01 20:29:18  What a Ben Simmons trade would mean for 76ers:...   \n",
       "2021-09-01 20:27:48  It’s Election Season in Germany. No Charisma, ...   \n",
       "2021-09-01 20:15:58  Vaccinated People and Breakthrough Infections:...   \n",
       "2021-09-01 20:04:38  Biden: Texas abortion law 'blatantly violates'...   \n",
       "2021-09-01 19:39:00  NeNe Leakes’ husband Gregg dies of cancer at 6...   \n",
       "2021-09-01 19:32:08  Psaki dodges questions about Biden pressing Af...   \n",
       "2021-09-01 19:14:00  Several California Public School Students Are ...   \n",
       "2021-09-01 19:10:00  The Apple Watch May Get Blood Pressure and Fer...   \n",
       "2021-09-01 18:56:01  N.J. weather: Tornado watch issued for 15 coun...   \n",
       "2021-09-01 18:55:26  Apple reportedly asks US employees to share th...   \n",
       "2021-09-01 18:45:24  Florida photographer befriends, documents firs...   \n",
       "2021-09-01 18:42:22  Oregon McDonald’s asks 14-year-olds to apply a...   \n",
       "2021-09-01 18:26:59  Cases of Mu COVID Variant That May Evade Vacci...   \n",
       "2021-09-01 18:24:35  Mars rover Perseverance set for 2nd sample-col...   \n",
       "2021-09-01 18:00:04  Grand Jury Indicts Three Aurora Cops, Two Para...   \n",
       "2021-09-01 18:00:00  ‘Top Gun: Maverick’ Flies From Thanksgiving To...   \n",
       "2021-08-29 09:00:15  As Washington Stews, State Legislatures Increa...   \n",
       "2021-08-29 09:00:15  When the New Covid Surge Struck, Mississippi W...   \n",
       "2021-08-31 16:15:55  GOP Governors Fight Virus Mandates as the Part...   \n",
       "2021-09-01 04:00:20  Can David Frost actually make Brexit work? Pol...   \n",
       "2021-08-29 17:37:42  Green politics is beset by a fundamental parad...   \n",
       "2021-09-01 13:18:22  Analysis: The Texas abortion law is a reminder...   \n",
       "2021-08-31 18:00:35  The Guardian view on the politics of work: new...   \n",
       "2021-08-30 10:00:31  Is the anxiety over New York schools reopening...   \n",
       "2021-08-31 16:24:01  Facebook will reduce political content in the ...   \n",
       "2021-08-30 23:01:07  Women’s Super League 2021-22 previews No 5: Ch...   \n",
       "2021-08-30 11:00:00  How to Prepare for Your Eventual Return to the...   \n",
       "2021-09-01 00:42:33  Developer Charged in Scheme to Bilk Matt Gaetz...   \n",
       "2021-09-01 11:00:28  What UK progressives can learn from community ...   \n",
       "2021-08-31 16:11:29  As Covid recedes in the UK, Boris Johnson is a...   \n",
       "2021-08-29 15:56:38  FIRST READING: Protesters dog Trudeau campaign...   \n",
       "2021-08-29 17:40:06  Trudeau vows to regulate oil and gas emissions...   \n",
       "2021-08-30 16:04:14  A Compass for the Politics of Collapse: A Shor...   \n",
       "2021-09-01 20:00:04  Keir Starmer urged to create ‘political cabine...   \n",
       "2021-08-31 12:30:03  Flipboard wants to make a better news feed by ...   \n",
       "2021-08-30 18:04:37  Conservative Party drops Nova Scotia candidate...   \n",
       "\n",
       "                                                                author  \\\n",
       "publishedAt                                                              \n",
       "2021-09-01 21:57:13                                      Alyssa Lukpat   \n",
       "2021-09-01 21:13:20                                    Danielle DuClos   \n",
       "2021-09-01 20:50:36                   https://www.facebook.com/bbcnews   \n",
       "2021-09-01 20:42:53                                        Jan Hoffman   \n",
       "2021-09-01 20:29:18                                                      \n",
       "2021-09-01 20:27:48                                    Katrin Bennhold   \n",
       "2021-09-01 20:15:58                                   Tara Parker-Pope   \n",
       "2021-09-01 20:04:38                                       Kelly Hooper   \n",
       "2021-09-01 19:39:00                                        Leah Bitsky   \n",
       "2021-09-01 19:32:08                                    Jessica Chasmar   \n",
       "2021-09-01 19:14:00                                     Deepa Shivaram   \n",
       "2021-09-01 19:10:00                                      Victoria Song   \n",
       "2021-09-01 18:56:01        Len Melisurgo | NJ Advance Media for NJ.com   \n",
       "2021-09-01 18:55:26  https://www.engadget.com/about/editors/igor-bo...   \n",
       "2021-09-01 18:45:24                                       Emilee Speck   \n",
       "2021-09-01 18:42:22                                      New York Post   \n",
       "2021-09-01 18:26:59                                         Jenni Fink   \n",
       "2021-09-01 18:24:35                                          Mike Wall   \n",
       "2021-09-01 18:00:04                                       Zoe Richards   \n",
       "2021-09-01 18:00:00                               Anthony D'Alessandro   \n",
       "2021-08-29 09:00:15                                      Michael Wines   \n",
       "2021-08-29 09:00:15                                    Richard Fausset   \n",
       "2021-08-31 16:15:55                                   Shane Goldmacher   \n",
       "2021-09-01 04:00:20  Presented by Jessica Elgot. With David Henig, ...   \n",
       "2021-08-29 17:37:42                                            Letters   \n",
       "2021-09-01 13:18:22    Analysis by Chris Cillizza, CNN Editor-at-large   \n",
       "2021-08-31 18:00:35                                          Editorial   \n",
       "2021-08-30 10:00:31                                       Emma Brockes   \n",
       "2021-08-31 16:24:01                                          Kris Holt   \n",
       "2021-08-30 23:01:07                    Louise Taylor and Sarah Rendell   \n",
       "2021-08-30 11:00:00                                 Lisa Rabasca Roepe   \n",
       "2021-09-01 00:42:33                                 Michael S. Schmidt   \n",
       "2021-09-01 11:00:28                  Ellie Mae O’Hagan and Josh Simons   \n",
       "2021-08-31 16:11:29                                        Rafael Behr   \n",
       "2021-08-29 15:56:38                                     Tristin Hopper   \n",
       "2021-08-29 17:40:06                                              Staff   \n",
       "2021-08-30 16:04:14                                     Conjure Utopia   \n",
       "2021-09-01 20:00:04                                    Heather Stewart   \n",
       "2021-08-31 12:30:03                                          Ian Sherr   \n",
       "2021-08-30 18:04:37                                               None   \n",
       "\n",
       "                                                                   url  \\\n",
       "publishedAt                                                              \n",
       "2021-09-01 21:57:13  https://www.nytimes.com/2021/09/01/business/jo...   \n",
       "2021-09-01 21:13:20  https://abcnews.go.com/Politics/social-securit...   \n",
       "2021-09-01 20:50:36  https://www.bbc.com/news/world-asia-india-5841...   \n",
       "2021-09-01 20:42:53  https://www.nytimes.com/2021/09/01/health/purd...   \n",
       "2021-09-01 20:29:18  https://www.cbssports.com/nba/news/what-a-ben-...   \n",
       "2021-09-01 20:27:48  https://www.nytimes.com/2021/09/01/world/europ...   \n",
       "2021-09-01 20:15:58  https://www.nytimes.com/article/delta-breakthr...   \n",
       "2021-09-01 20:04:38  https://www.politico.com/news/2021/09/01/biden...   \n",
       "2021-09-01 19:39:00  https://pagesix.com/2021/09/01/nene-leakes-hus...   \n",
       "2021-09-01 19:32:08  https://www.foxnews.com/politics/psaki-dodges-...   \n",
       "2021-09-01 19:14:00  https://www.npr.org/2021/09/01/1033294030/amer...   \n",
       "2021-09-01 19:10:00  https://gizmodo.com/the-apple-watch-may-get-bl...   \n",
       "2021-09-01 18:56:01  https://www.nj.com/weather/2021/09/nj-weather-...   \n",
       "2021-09-01 18:55:26  https://www.engadget.com/apple-vaccination-sta...   \n",
       "2021-09-01 18:45:24  https://www.clickorlando.com/news/local/2021/0...   \n",
       "2021-09-01 18:42:22  https://www.foxbusiness.com/lifestyle/oregon-m...   \n",
       "2021-09-01 18:26:59  https://www.newsweek.com/cases-mu-covid-varian...   \n",
       "2021-09-01 18:24:35  https://www.space.com/mars-rover-perseverance-...   \n",
       "2021-09-01 18:00:04  https://www.thedailybeast.com/grand-jury-indic...   \n",
       "2021-09-01 18:00:00  https://deadline.com/2021/09/top-gun-maverick-...   \n",
       "2021-08-29 09:00:15  https://www.nytimes.com/2021/08/29/us/state-le...   \n",
       "2021-08-29 09:00:15  https://www.nytimes.com/2021/08/29/us/when-the...   \n",
       "2021-08-31 16:15:55  https://www.nytimes.com/2021/08/31/us/politics...   \n",
       "2021-09-01 04:00:20  https://www.theguardian.com/politics/audio/202...   \n",
       "2021-08-29 17:37:42  https://amp.theguardian.com/politics/2021/aug/...   \n",
       "2021-09-01 13:18:22  https://www.cnn.com/2021/09/01/politics/democr...   \n",
       "2021-08-31 18:00:35  https://amp.theguardian.com/commentisfree/2021...   \n",
       "2021-08-30 10:00:31  https://amp.theguardian.com/commentisfree/2021...   \n",
       "2021-08-31 16:24:01  https://www.engadget.com/facebook-political-co...   \n",
       "2021-08-30 23:01:07  https://amp.theguardian.com/football/2021/aug/...   \n",
       "2021-08-30 11:00:00  https://www.wired.com/story/how-to-prepare-ret...   \n",
       "2021-09-01 00:42:33  https://www.nytimes.com/2021/09/01/us/politics...   \n",
       "2021-09-01 11:00:28  https://amp.theguardian.com/commentisfree/2021...   \n",
       "2021-08-31 16:11:29  https://amp.theguardian.com/commentisfree/2021...   \n",
       "2021-08-29 15:56:38  https://nationalpost.com/news/canada/first-rea...   \n",
       "2021-08-29 17:40:06  https://globalnews.ca/news/8150115/liberals-oi...   \n",
       "2021-08-30 16:04:14  https://write.as/conjure-utopia/a-compass-for-...   \n",
       "2021-09-01 20:00:04  https://amp.theguardian.com/politics/2021/sep/...   \n",
       "2021-08-31 12:30:03  https://www.cnet.com/tech/mobile/flipboard-wan...   \n",
       "2021-08-30 18:04:37  https://www.theglobeandmail.com/politics/artic...   \n",
       "\n",
       "                                                                  text  \\\n",
       "publishedAt                                                              \n",
       "2021-09-01 21:57:13  joe  rogan  a  podcasting  giant  who  has  be...   \n",
       "2021-09-01 21:13:20  social  security costs to exceed revenue for  ...   \n",
       "2021-09-01 20:50:36  syed  ali  geelani  kashmir is separatist patr...   \n",
       "2021-09-01 20:42:53  purdue  pharma  is  dissolved and  sacklers  p...   \n",
       "2021-09-01 20:29:18  what a  ben  simmons trade would mean for   a ...   \n",
       "2021-09-01 20:27:48  its  election  season in  germany.  no  charis...   \n",
       "2021-09-01 20:15:58  what  vaccinated  people  need to  know  about...   \n",
       "2021-09-01 20:04:38  biden  texas abortion law blatantly violates  ...   \n",
       "2021-09-01 19:39:00  ne ne  leakes husband  gregg dies of cancer at...   \n",
       "2021-09-01 19:32:08  psaki dodges questions about  biden pressing  ...   \n",
       "2021-09-01 19:14:00  several  american  students  are  still  trapp...   \n",
       "2021-09-01 19:10:00  apple  watch  may  get  blood  pressure and  f...   \n",
       "2021-09-01 18:56:01  n.j. weather  tornado watch issued for  counti...   \n",
       "2021-09-01 18:55:26  apple reportedly asks  us employees to share t...   \n",
       "2021-09-01 18:45:24  florida photographer befriends documents first...   \n",
       "2021-09-01 18:42:22  oregon  mc donalds asks  to apply amid labor s...   \n",
       "2021-09-01 18:26:59                                          forbidden   \n",
       "2021-09-01 18:24:35  mars rover  perseverance set for  samplecollec...   \n",
       "2021-09-01 18:00:04  grand  jury  indicts  three  aurora  cops  two...   \n",
       "2021-09-01 18:00:00   top  gun  maverick  opening  memorial  day  w...   \n",
       "2021-08-29 09:00:15  as  washington  stews  state  legislatures  in...   \n",
       "2021-08-29 09:00:15  when the  new  covid  surge  struck  mississip...   \n",
       "2021-08-31 16:15:55  gop  governors  fight  virus  mandates as the ...   \n",
       "2021-09-01 04:00:20  can  david  frost actually make  brexit work  ...   \n",
       "2021-08-29 17:37:42  green politics is beset by a fundamental parad...   \n",
       "2021-09-01 13:18:22  analysis  all eyes on  breyer after  supreme  ...   \n",
       "2021-08-31 18:00:35  the  guardian view on the politics of work new...   \n",
       "2021-08-30 10:00:31  is the anxiety over  new  york schools reopeni...   \n",
       "2021-08-31 16:24:01  facebook will reduce political content in the ...   \n",
       "2021-08-30 23:01:07  womens  super  league  previews  no   chelsea ...   \n",
       "2021-08-30 11:00:00  how to  prepare for  your  eventual  return to...   \n",
       "2021-09-01 00:42:33  developer  charged in  scheme to  bilk  matt  ...   \n",
       "2021-09-01 11:00:28  what  uk progressives can learn from community...   \n",
       "2021-08-31 16:11:29  without a guiding purpose  boris  johnson will...   \n",
       "2021-08-29 15:56:38  first reading  protesters dog  trudeau campaig...   \n",
       "2021-08-29 17:40:06  trudeau vows to regulate oil and gas emissions...   \n",
       "2021-08-30 16:04:14  a  compass for the  politics of  collapse  a  ...   \n",
       "2021-09-01 20:00:04  keir  starmer urged to create political cabine...   \n",
       "2021-08-31 12:30:03  flipboard wants to make a better news feed by ...   \n",
       "2021-08-30 18:04:37  conservative  party drops  nova  scotia candid...   \n",
       "\n",
       "                                               keywords  \n",
       "publishedAt                                              \n",
       "2021-09-01 21:57:13         (sections, search, content)  \n",
       "2021-09-01 21:13:20     (majority, security, projected)  \n",
       "2021-09-01 20:50:36       (kashmir, external, pakistan)  \n",
       "2021-09-01 20:42:53       (sacklers, bankruptcy, judge)  \n",
       "2021-09-01 20:29:18             (sixers, jasmyn, quinn)  \n",
       "2021-09-01 20:27:48           (scholz, please!, merkel)  \n",
       "2021-09-01 20:15:58       (people, experts, vaccinated)  \n",
       "2021-09-01 20:04:38             (texas, magazine, skip)  \n",
       "2021-09-01 19:39:00            (leakes, click, husband)  \n",
       "2021-09-01 19:32:08                (news, psaki, biden)  \n",
       "2021-09-01 19:14:00      (expandcollapse, juan, karzai)  \n",
       "2021-09-01 19:10:00          (gizmodo, exploring, club)  \n",
       "2021-09-01 18:56:01               (jersey, york, watch)  \n",
       "2021-09-01 18:55:26     (verizon, reviews, vaccination)  \n",
       "2021-09-01 18:45:24                (space, kraus, zero)  \n",
       "2021-09-01 18:42:22            (jobs, business, policy)  \n",
       "2021-09-01 18:26:59                         (forbidden)  \n",
       "2021-09-01 18:24:35         (next, perseverance, more!)  \n",
       "2021-09-01 18:00:04             (clain, elijah, police)  \n",
       "2021-09-01 18:00:00         (expand, maverick, weekend)  \n",
       "2021-08-29 09:00:15              (texas, like, arizona)  \n",
       "2021-08-29 09:00:15           (uniquely, york, jackson)  \n",
       "2021-08-31 16:15:55  (republican, requiring, governors)  \n",
       "2021-09-01 04:00:20         (guardian, switch, puzzles)  \n",
       "2021-08-29 17:37:42               (jobs, paradox, home)  \n",
       "2021-09-01 13:18:22            (texas, justice, breyer)  \n",
       "2021-08-31 18:00:35                  (next, jobs, home)  \n",
       "2021-08-30 10:00:31              (anxiety, home, sport)  \n",
       "2021-08-31 16:24:01           (facebook, verizon, home)  \n",
       "2021-08-30 23:01:07             (squad, previews, home)  \n",
       "2021-08-30 11:00:00                 (work, back, quick)  \n",
       "2021-09-01 00:42:33              (justice, gaetz, bilk)  \n",
       "2021-09-01 11:00:28         (arizona, jobs, experience)  \n",
       "2021-08-31 16:11:29           (johnson, johnsons, home)  \n",
       "2021-08-29 15:56:38   (advertisement, campaign, justin)  \n",
       "2021-08-29 17:40:06         (facebook, require, justin)  \n",
       "2021-08-30 16:04:14       (examples, collapse, conjure)  \n",
       "2021-09-01 20:00:04            (experts, home, opinion)  \n",
       "2021-08-31 12:30:03         (facebook, tesla, roadster)  \n",
       "2021-08-30 18:04:37              (sexual, party, globe)  "
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>author</th>\n",
       "      <th>url</th>\n",
       "      <th>text</th>\n",
       "      <th>keywords</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>publishedAt</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2021-09-01 21:57:13</th>\n",
       "      <td>Joe Rogan, A Podcasting Giant Who Has Been Dis...</td>\n",
       "      <td>Alyssa Lukpat</td>\n",
       "      <td>https://www.nytimes.com/2021/09/01/business/jo...</td>\n",
       "      <td>joe  rogan  a  podcasting  giant  who  has  be...</td>\n",
       "      <td>(sections, search, content)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021-09-01 21:13:20</th>\n",
       "      <td>Social Security costs to exceed revenue for 1s...</td>\n",
       "      <td>Danielle DuClos</td>\n",
       "      <td>https://abcnews.go.com/Politics/social-securit...</td>\n",
       "      <td>social  security costs to exceed revenue for  ...</td>\n",
       "      <td>(majority, security, projected)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021-09-01 20:50:36</th>\n",
       "      <td>Syed Ali Geelani: Kashmir's separatist patriar...</td>\n",
       "      <td>https://www.facebook.com/bbcnews</td>\n",
       "      <td>https://www.bbc.com/news/world-asia-india-5841...</td>\n",
       "      <td>syed  ali  geelani  kashmir is separatist patr...</td>\n",
       "      <td>(kashmir, external, pakistan)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021-09-01 20:42:53</th>\n",
       "      <td>Purdue Pharma Is Dissolved and Sacklers Pay $4...</td>\n",
       "      <td>Jan Hoffman</td>\n",
       "      <td>https://www.nytimes.com/2021/09/01/health/purd...</td>\n",
       "      <td>purdue  pharma  is  dissolved and  sacklers  p...</td>\n",
       "      <td>(sacklers, bankruptcy, judge)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021-09-01 20:29:18</th>\n",
       "      <td>What a Ben Simmons trade would mean for 76ers:...</td>\n",
       "      <td></td>\n",
       "      <td>https://www.cbssports.com/nba/news/what-a-ben-...</td>\n",
       "      <td>what a  ben  simmons trade would mean for   a ...</td>\n",
       "      <td>(sixers, jasmyn, quinn)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021-09-01 20:27:48</th>\n",
       "      <td>It’s Election Season in Germany. No Charisma, ...</td>\n",
       "      <td>Katrin Bennhold</td>\n",
       "      <td>https://www.nytimes.com/2021/09/01/world/europ...</td>\n",
       "      <td>its  election  season in  germany.  no  charis...</td>\n",
       "      <td>(scholz, please!, merkel)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021-09-01 20:15:58</th>\n",
       "      <td>Vaccinated People and Breakthrough Infections:...</td>\n",
       "      <td>Tara Parker-Pope</td>\n",
       "      <td>https://www.nytimes.com/article/delta-breakthr...</td>\n",
       "      <td>what  vaccinated  people  need to  know  about...</td>\n",
       "      <td>(people, experts, vaccinated)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021-09-01 20:04:38</th>\n",
       "      <td>Biden: Texas abortion law 'blatantly violates'...</td>\n",
       "      <td>Kelly Hooper</td>\n",
       "      <td>https://www.politico.com/news/2021/09/01/biden...</td>\n",
       "      <td>biden  texas abortion law blatantly violates  ...</td>\n",
       "      <td>(texas, magazine, skip)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021-09-01 19:39:00</th>\n",
       "      <td>NeNe Leakes’ husband Gregg dies of cancer at 6...</td>\n",
       "      <td>Leah Bitsky</td>\n",
       "      <td>https://pagesix.com/2021/09/01/nene-leakes-hus...</td>\n",
       "      <td>ne ne  leakes husband  gregg dies of cancer at...</td>\n",
       "      <td>(leakes, click, husband)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021-09-01 19:32:08</th>\n",
       "      <td>Psaki dodges questions about Biden pressing Af...</td>\n",
       "      <td>Jessica Chasmar</td>\n",
       "      <td>https://www.foxnews.com/politics/psaki-dodges-...</td>\n",
       "      <td>psaki dodges questions about  biden pressing  ...</td>\n",
       "      <td>(news, psaki, biden)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021-09-01 19:14:00</th>\n",
       "      <td>Several California Public School Students Are ...</td>\n",
       "      <td>Deepa Shivaram</td>\n",
       "      <td>https://www.npr.org/2021/09/01/1033294030/amer...</td>\n",
       "      <td>several  american  students  are  still  trapp...</td>\n",
       "      <td>(expandcollapse, juan, karzai)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021-09-01 19:10:00</th>\n",
       "      <td>The Apple Watch May Get Blood Pressure and Fer...</td>\n",
       "      <td>Victoria Song</td>\n",
       "      <td>https://gizmodo.com/the-apple-watch-may-get-bl...</td>\n",
       "      <td>apple  watch  may  get  blood  pressure and  f...</td>\n",
       "      <td>(gizmodo, exploring, club)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021-09-01 18:56:01</th>\n",
       "      <td>N.J. weather: Tornado watch issued for 15 coun...</td>\n",
       "      <td>Len Melisurgo | NJ Advance Media for NJ.com</td>\n",
       "      <td>https://www.nj.com/weather/2021/09/nj-weather-...</td>\n",
       "      <td>n.j. weather  tornado watch issued for  counti...</td>\n",
       "      <td>(jersey, york, watch)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021-09-01 18:55:26</th>\n",
       "      <td>Apple reportedly asks US employees to share th...</td>\n",
       "      <td>https://www.engadget.com/about/editors/igor-bo...</td>\n",
       "      <td>https://www.engadget.com/apple-vaccination-sta...</td>\n",
       "      <td>apple reportedly asks  us employees to share t...</td>\n",
       "      <td>(verizon, reviews, vaccination)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021-09-01 18:45:24</th>\n",
       "      <td>Florida photographer befriends, documents firs...</td>\n",
       "      <td>Emilee Speck</td>\n",
       "      <td>https://www.clickorlando.com/news/local/2021/0...</td>\n",
       "      <td>florida photographer befriends documents first...</td>\n",
       "      <td>(space, kraus, zero)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021-09-01 18:42:22</th>\n",
       "      <td>Oregon McDonald’s asks 14-year-olds to apply a...</td>\n",
       "      <td>New York Post</td>\n",
       "      <td>https://www.foxbusiness.com/lifestyle/oregon-m...</td>\n",
       "      <td>oregon  mc donalds asks  to apply amid labor s...</td>\n",
       "      <td>(jobs, business, policy)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021-09-01 18:26:59</th>\n",
       "      <td>Cases of Mu COVID Variant That May Evade Vacci...</td>\n",
       "      <td>Jenni Fink</td>\n",
       "      <td>https://www.newsweek.com/cases-mu-covid-varian...</td>\n",
       "      <td>forbidden</td>\n",
       "      <td>(forbidden)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021-09-01 18:24:35</th>\n",
       "      <td>Mars rover Perseverance set for 2nd sample-col...</td>\n",
       "      <td>Mike Wall</td>\n",
       "      <td>https://www.space.com/mars-rover-perseverance-...</td>\n",
       "      <td>mars rover  perseverance set for  samplecollec...</td>\n",
       "      <td>(next, perseverance, more!)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021-09-01 18:00:04</th>\n",
       "      <td>Grand Jury Indicts Three Aurora Cops, Two Para...</td>\n",
       "      <td>Zoe Richards</td>\n",
       "      <td>https://www.thedailybeast.com/grand-jury-indic...</td>\n",
       "      <td>grand  jury  indicts  three  aurora  cops  two...</td>\n",
       "      <td>(clain, elijah, police)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021-09-01 18:00:00</th>\n",
       "      <td>‘Top Gun: Maverick’ Flies From Thanksgiving To...</td>\n",
       "      <td>Anthony D'Alessandro</td>\n",
       "      <td>https://deadline.com/2021/09/top-gun-maverick-...</td>\n",
       "      <td>top  gun  maverick  opening  memorial  day  w...</td>\n",
       "      <td>(expand, maverick, weekend)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021-08-29 09:00:15</th>\n",
       "      <td>As Washington Stews, State Legislatures Increa...</td>\n",
       "      <td>Michael Wines</td>\n",
       "      <td>https://www.nytimes.com/2021/08/29/us/state-le...</td>\n",
       "      <td>as  washington  stews  state  legislatures  in...</td>\n",
       "      <td>(texas, like, arizona)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021-08-29 09:00:15</th>\n",
       "      <td>When the New Covid Surge Struck, Mississippi W...</td>\n",
       "      <td>Richard Fausset</td>\n",
       "      <td>https://www.nytimes.com/2021/08/29/us/when-the...</td>\n",
       "      <td>when the  new  covid  surge  struck  mississip...</td>\n",
       "      <td>(uniquely, york, jackson)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021-08-31 16:15:55</th>\n",
       "      <td>GOP Governors Fight Virus Mandates as the Part...</td>\n",
       "      <td>Shane Goldmacher</td>\n",
       "      <td>https://www.nytimes.com/2021/08/31/us/politics...</td>\n",
       "      <td>gop  governors  fight  virus  mandates as the ...</td>\n",
       "      <td>(republican, requiring, governors)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021-09-01 04:00:20</th>\n",
       "      <td>Can David Frost actually make Brexit work? Pol...</td>\n",
       "      <td>Presented by Jessica Elgot. With David Henig, ...</td>\n",
       "      <td>https://www.theguardian.com/politics/audio/202...</td>\n",
       "      <td>can  david  frost actually make  brexit work  ...</td>\n",
       "      <td>(guardian, switch, puzzles)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021-08-29 17:37:42</th>\n",
       "      <td>Green politics is beset by a fundamental parad...</td>\n",
       "      <td>Letters</td>\n",
       "      <td>https://amp.theguardian.com/politics/2021/aug/...</td>\n",
       "      <td>green politics is beset by a fundamental parad...</td>\n",
       "      <td>(jobs, paradox, home)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021-09-01 13:18:22</th>\n",
       "      <td>Analysis: The Texas abortion law is a reminder...</td>\n",
       "      <td>Analysis by Chris Cillizza, CNN Editor-at-large</td>\n",
       "      <td>https://www.cnn.com/2021/09/01/politics/democr...</td>\n",
       "      <td>analysis  all eyes on  breyer after  supreme  ...</td>\n",
       "      <td>(texas, justice, breyer)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021-08-31 18:00:35</th>\n",
       "      <td>The Guardian view on the politics of work: new...</td>\n",
       "      <td>Editorial</td>\n",
       "      <td>https://amp.theguardian.com/commentisfree/2021...</td>\n",
       "      <td>the  guardian view on the politics of work new...</td>\n",
       "      <td>(next, jobs, home)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021-08-30 10:00:31</th>\n",
       "      <td>Is the anxiety over New York schools reopening...</td>\n",
       "      <td>Emma Brockes</td>\n",
       "      <td>https://amp.theguardian.com/commentisfree/2021...</td>\n",
       "      <td>is the anxiety over  new  york schools reopeni...</td>\n",
       "      <td>(anxiety, home, sport)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021-08-31 16:24:01</th>\n",
       "      <td>Facebook will reduce political content in the ...</td>\n",
       "      <td>Kris Holt</td>\n",
       "      <td>https://www.engadget.com/facebook-political-co...</td>\n",
       "      <td>facebook will reduce political content in the ...</td>\n",
       "      <td>(facebook, verizon, home)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021-08-30 23:01:07</th>\n",
       "      <td>Women’s Super League 2021-22 previews No 5: Ch...</td>\n",
       "      <td>Louise Taylor and Sarah Rendell</td>\n",
       "      <td>https://amp.theguardian.com/football/2021/aug/...</td>\n",
       "      <td>womens  super  league  previews  no   chelsea ...</td>\n",
       "      <td>(squad, previews, home)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021-08-30 11:00:00</th>\n",
       "      <td>How to Prepare for Your Eventual Return to the...</td>\n",
       "      <td>Lisa Rabasca Roepe</td>\n",
       "      <td>https://www.wired.com/story/how-to-prepare-ret...</td>\n",
       "      <td>how to  prepare for  your  eventual  return to...</td>\n",
       "      <td>(work, back, quick)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021-09-01 00:42:33</th>\n",
       "      <td>Developer Charged in Scheme to Bilk Matt Gaetz...</td>\n",
       "      <td>Michael S. Schmidt</td>\n",
       "      <td>https://www.nytimes.com/2021/09/01/us/politics...</td>\n",
       "      <td>developer  charged in  scheme to  bilk  matt  ...</td>\n",
       "      <td>(justice, gaetz, bilk)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021-09-01 11:00:28</th>\n",
       "      <td>What UK progressives can learn from community ...</td>\n",
       "      <td>Ellie Mae O’Hagan and Josh Simons</td>\n",
       "      <td>https://amp.theguardian.com/commentisfree/2021...</td>\n",
       "      <td>what  uk progressives can learn from community...</td>\n",
       "      <td>(arizona, jobs, experience)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021-08-31 16:11:29</th>\n",
       "      <td>As Covid recedes in the UK, Boris Johnson is a...</td>\n",
       "      <td>Rafael Behr</td>\n",
       "      <td>https://amp.theguardian.com/commentisfree/2021...</td>\n",
       "      <td>without a guiding purpose  boris  johnson will...</td>\n",
       "      <td>(johnson, johnsons, home)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021-08-29 15:56:38</th>\n",
       "      <td>FIRST READING: Protesters dog Trudeau campaign...</td>\n",
       "      <td>Tristin Hopper</td>\n",
       "      <td>https://nationalpost.com/news/canada/first-rea...</td>\n",
       "      <td>first reading  protesters dog  trudeau campaig...</td>\n",
       "      <td>(advertisement, campaign, justin)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021-08-29 17:40:06</th>\n",
       "      <td>Trudeau vows to regulate oil and gas emissions...</td>\n",
       "      <td>Staff</td>\n",
       "      <td>https://globalnews.ca/news/8150115/liberals-oi...</td>\n",
       "      <td>trudeau vows to regulate oil and gas emissions...</td>\n",
       "      <td>(facebook, require, justin)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021-08-30 16:04:14</th>\n",
       "      <td>A Compass for the Politics of Collapse: A Shor...</td>\n",
       "      <td>Conjure Utopia</td>\n",
       "      <td>https://write.as/conjure-utopia/a-compass-for-...</td>\n",
       "      <td>a  compass for the  politics of  collapse  a  ...</td>\n",
       "      <td>(examples, collapse, conjure)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021-09-01 20:00:04</th>\n",
       "      <td>Keir Starmer urged to create ‘political cabine...</td>\n",
       "      <td>Heather Stewart</td>\n",
       "      <td>https://amp.theguardian.com/politics/2021/sep/...</td>\n",
       "      <td>keir  starmer urged to create political cabine...</td>\n",
       "      <td>(experts, home, opinion)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021-08-31 12:30:03</th>\n",
       "      <td>Flipboard wants to make a better news feed by ...</td>\n",
       "      <td>Ian Sherr</td>\n",
       "      <td>https://www.cnet.com/tech/mobile/flipboard-wan...</td>\n",
       "      <td>flipboard wants to make a better news feed by ...</td>\n",
       "      <td>(facebook, tesla, roadster)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021-08-30 18:04:37</th>\n",
       "      <td>Conservative Party drops Nova Scotia candidate...</td>\n",
       "      <td>None</td>\n",
       "      <td>https://www.theglobeandmail.com/politics/artic...</td>\n",
       "      <td>conservative  party drops  nova  scotia candid...</td>\n",
       "      <td>(sexual, party, globe)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "metadata": {},
     "execution_count": 13
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 4. Get Important Words & Phrases"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 5. Search Twitter API\n",
    "## Using Important Words & Phrases"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Create Tweets class"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "source": [
    "class Tweets():\n",
    "\n",
    "    def __init__(self, consumer_key, consumer_secret, access_token, access_token_secret, logger=logging):\n",
    "        self.logger = logging.basicConfig(filename='tweets.log', filemode='w',\n",
    "                                          format=f'%(asctime)s - %(levelname)s - %(message)s')\n",
    "        self.consumer_key = consumer_key\n",
    "        self.consumer_secret = consumer_secret\n",
    "        self.access_token = access_token\n",
    "        self.access_token_secret = access_token_secret\n",
    "\n",
    "        self.location = sys.argv[1]  # user location as argument variable\n",
    "        # object with latitude & longitude of user location\n",
    "        self.geo = geocoder.osm(self.location)\n",
    "\n",
    "    def tweepy_auth(self):\n",
    "        \"\"\"Authorize tweepy API\"\"\"\n",
    "\n",
    "        self.auth = tweepy.OAuthHandler(self.consumer_key, self.consumer_secret)\n",
    "        self.auth.set_access_token(self.access_token, self.access_token_secret)\n",
    "\n",
    "        # create API object\n",
    "        self.api = API(self.auth, wait_on_rate_limit=True, user_agent=get_random_ua('Chrome'))# wait_on_rate_limit_notify=True)\n",
    "\n",
    "        try:\n",
    "            self.api.verify_credentials()\n",
    "            logging.info(\"Tweepy API Authenticated\")\n",
    "            print('Tweepy authentication successful')\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Error during Tweepy authentication: {e}\")\n",
    "            raise e\n",
    "        return self.api\n",
    "    \n",
    "    def get_tweets(self, news_keywords, news_instance): # TODO add stream listening stuff to params\n",
    "        searched_tweets = self.tweet_search(news_keywords)\n",
    "        stream_tweets = TwitterStreamListener.on_status(listener, tweet_stream)\n",
    "\n",
    "        # all_tweets = {}\n",
    "        # # process tweets\n",
    "        # for tweet in searched_tweets:\n",
    "        #     # count tweets\n",
    "        #     pass\n",
    "        #     # add count to df column?\n",
    "            \n",
    "        # for tweet in stream_tweets:\n",
    "        #     pass\n",
    "        # # break tweets apart for table\n",
    "        # for tweet in searched_tweets, stream_tweets:\n",
    "        #     all_tweets[\"tweet_id\"] = tweet['id']\n",
    "\n",
    "        #     # add all tweets to database! via mogrify\n",
    "\n",
    "        #     # put tweets in df\n",
    "        #     self.all_tweets_df = pd.DataFrame.from_dict(all_tweets, columns=[\n",
    "        #                                               \"tweet_id\", \"user_id\", \"location\", \"createdAt\", \"tweet_text\"])\n",
    "\n",
    "        #     self.all_tweets_df.set_index(\"tweet_id\")\n",
    "\n",
    "        #     # tweets mention count to news df column\n",
    "        #     news_instance.all_news_df[\"tweet_mention_count\"] = self.all_tweets_df[\"tweet_id\"].apply(\n",
    "        #         np.count_nonzero)\n",
    "\n",
    "            # clear dataframe?\n",
    "    \n",
    "\n",
    "    def tweet_search(self, news_keywords):\n",
    "        \"\"\"Search for tweets within previous 7 days.\n",
    "                    Inputs:\n",
    "                        keyword list\n",
    "                    Returns:\n",
    "                        Tweet list => JSON\n",
    "        \"\"\"\n",
    "        api = self.api\n",
    "        self.search_tweet_count = 0\n",
    "\n",
    "        # unpack keyword tuples\n",
    "        print('Searching for tweets matching keywords')\n",
    "        for keys in news_keywords:\n",
    "            keywords = list(keys)  # TODO add itertools combinations\n",
    "            for word in keywords:\n",
    "                try:\n",
    "                    result = api.search_tweets(q=str(\n",
    "                                    word) + \" -filter:retweets\", lang='en')\n",
    "                                # print(type(result))\n",
    "                    status = result[0]\n",
    "                                # print(type(status))\n",
    "                    tweet = status._json\n",
    "                    self.search_tweet_count = len(tweet)\n",
    "                                #self.file.write(json.dumps(tweets)+ '\\\\n')\n",
    "                    tweet = json.dumps(tweet)  # tweet to json string\n",
    "                    # assert (type(tweet) == str), \"Tweet must be converted to JSON string\"\n",
    "                    tweet = json.loads(tweet)  # tweet to dict\n",
    "                    # assert (type(tweet) == dict), \"Tweet must be converted from JSON string to type dict\"\n",
    "                except (TypeError) as e:\n",
    "                    logging('Error: ', e)\n",
    "                    print('Error: keyword not found in tweet search')\n",
    "                    print(self.search_tweet_count)\n",
    "                    break\n",
    "                else:\n",
    "                    # write tweets to json file\n",
    "                    with open(\"tweets.json\", \"a\") as f:\n",
    "                        json.dump(tweet, f)\n",
    "        logging.info('Tweet search successful')\n",
    "        print(f'Tweet search by keyword was successful. Counted {self.search_tweet_count} tweets.')\n",
    "\n",
    "        #finally:\n",
    "        # TODO add tweet unpacking & cleaning?\n",
    "        #pass\n",
    "        # TODO put tweets into db\n",
    "        # TODO\n",
    "    \n",
    "    def clean_tweets(self, tweets):\n",
    "        # use slang.txt\n",
    "        # https://www.geeksforgeeks.org/python-efficient-text-data-cleaning/\n",
    "        pass\n",
    "\n",
    "\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "source": [
    "\n",
    "# news = News(news_api_key)\n",
    "t = Tweets(consumer_key, consumer_secret,access_token, access_token_secret)\n",
    "auth = t.tweepy_auth()\n"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Tweepy authentication successful\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# define stream listener class\n",
    "class TwitterStreamListener(tweepy.Stream):\n",
    "    def __init__(self, api=None):\n",
    "        super(tweepy.Stream, self).__init__()\n",
    "        # super(json.JSONEncoder, self).__init__()\n",
    "        self.consumer_key = consumer_key\n",
    "        self.consumer_secret = consumer_secret\n",
    "        self.access_token = access_token\n",
    "        self.access_token_secret = access_token_secret\n",
    "        self.api = api\n",
    "        \n",
    "        self.num_tweets = 0\n",
    "        # self.file = open('tweets.txt', 'w')\n",
    "        self.tweet_list = []\n",
    "    \n",
    "    def toJson(self):\n",
    "        return json.dumps(self, default=lambda o: o.__dict__)\n",
    "\n",
    "    def on_status(self, status): # returns JSON \n",
    "        #print(status[0]._json)\n",
    "\n",
    "        #tweet = status._json\n",
    "        #print(status.id)\n",
    "        # Retweet count\n",
    "        # retweet_count = status['retweet_count']\n",
    "        # status = status.toJson()\n",
    "        # status = json.loads(status)\n",
    "        # status = str(status)\n",
    "        \n",
    "\n",
    "        while self.num_tweets < 450:  # max stream rate is for the twitter API Client\n",
    "            # for tweet in status:\n",
    "            #     tweet = tweet._json\n",
    "            #     print(\"printing twetets\")\n",
    "            #     print(tweet)\n",
    "            try:\n",
    "               # self.filter()\n",
    "                with open('tweets.json', 'a') as f:\n",
    "                    # write results to JSON file\n",
    "                    print(\"Returning JSON-ish results as string\")\n",
    "                    json.dump(status._json, f)\n",
    "                    #f.write(tweet)\n",
    "                    #return True\n",
    "\n",
    "            except TypeError as e:\n",
    "                logging('Error: ', e)\n",
    "                print('{e}: Please convert Stream object')\n",
    "                continue\n",
    "            # TODO: except user exit or stream disconnect\n",
    "            else:\n",
    "                with open('tweets.json', 'r') as file:\n",
    "                    # create Python list object from JSON\n",
    "                    tweets_json = file.read().split(\"\\n\")\n",
    "\n",
    "                    for tweet in tweets_json:\n",
    "                        print(\"Deserializing tweets\")\n",
    "                        tweet_obj = json.loads(tweet)\n",
    "\n",
    "                        # Tweet ID\n",
    "                        tweet_obj['tweet_id'] = tweet_obj['id']\n",
    "                        # User ID\n",
    "                        tweet_obj['user_id'] = tweet_obj['user']['id']\n",
    "                        # Username\n",
    "                        tweet_obj['username'] = tweet_obj['user']['name']\n",
    "                        # creation date\n",
    "                        tweet_obj['create_time'] = tweet_obj['user']['creation_date']\n",
    "                        # Language\n",
    "                        lang = status['lang']\n",
    "\n",
    "                        # Tweet\n",
    "                        if status.truncated == True:\n",
    "                            tweet = tweet_obj['extended_tweet']['full_text']\n",
    "                            hashtags = tweet_obj['extended_tweet']['entities']['hashtags']\n",
    "                        else:\n",
    "                            tweet = status.text\n",
    "                            hashtags = status.entities['hashtags']\n",
    "\n",
    "                        # Read hashtags using helper function\n",
    "                        print(\"Reading hashtags\")\n",
    "                        hashtags = read_hashtags(hashtags)\n",
    "\n",
    "                        # add info to pop_news dict\n",
    "                        # If tweet is not a retweet and tweet is in English\n",
    "                        if not hasattr(status, \"retweeted_status\") and lang == \"en\":\n",
    "                            print(\"Adding tweets to list\")\n",
    "                            self.tweet_list.append(tweet_obj)\n",
    "                    \n",
    "                        #self.tweet_list.append(status)\n",
    "                            self.num_tweets += 1\n",
    "            finally:\n",
    "                self.disconnect()\n",
    "                return self.tweet_list\n",
    "                \n",
    "\n",
    "            # flatten data to dataframe\n",
    "            # tweets = pd.json_normalize(self.tweet_list, record_path=['articles'])\n",
    "        #self.tweets_df = pd.DataFrame(self.tweet_list, columns=[\n",
    "                                      #\"tweet_id\", \"publishedAt\", \"userID\", \"text\", \"location\"])\n",
    "    \n",
    "    def on_data(self, data):\n",
    "        # payload = {}\n",
    "        data = json.loads(data)\n",
    "        print(data)\n",
    "\n",
    "    # Extract hashtags\n",
    "    def read_hashtags(self, tag_list):\n",
    "        hashtags = []\n",
    "\n",
    "        for tag in tag_list:\n",
    "            hashtags.append(tag['text'])\n",
    "\n",
    "        return hashtags\n",
    "    \n",
    "    def clean_tweets(self):\n",
    "\n",
    "        with open(\"tweets.json\", \"w\") as f:\n",
    "            # write tweets to json file\n",
    "            json.dump(tweet, f)\n",
    "\n",
    "        with open(\"tweets.json\", \"r\") as file:\n",
    "            # create python object from json\n",
    "            tweets_json = file.read().split(\"\\n\")\n",
    "\n",
    "            for tweet in tweets_json:\n",
    "                tweet_obj = json.loads(tweet)\n",
    "\n",
    "                #flatten nested fields\n",
    "                if 'quoted_status' in tweet_obj:\n",
    "                    tweet_obj['quote_tweet'] = tweet_obj['quoted_status']['extended_tweet']['full_text']\n",
    "                if 'user' in tweet_obj:\n",
    "                    tweet_obj['location'] = tweet_obj['user']['location']\n",
    "                # if 'created_at' in tweet_obj:\n",
    "                #     tweet_obj['created_at'] = pd.to_datetime(tweet)\n",
    "\n",
    "    def on_error(self, status_code):\n",
    "        if status_code == 420:\n",
    "            return False  # false disconnects the stream\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "\n",
    "auth = t.tweepy_auth()\n",
    "# instantiate Tweet Stream Listener\n",
    "listener = TwitterStreamListener()\n",
    "# authenticate stream\n",
    "tweet_stream = tweepy.Stream(auth, listener, access_token, access_token_secret) #tweet_mode=\"extended\")\n",
    "#listener.on_status(tweet_stream)\n",
    "listener.on_data(tweet_stream)\n",
    "\n",
    "# print cleaned tweets df\n",
    "\n",
    "# print(news.all_news_df)\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 6. Search TikTok\n",
    "## Using Important Words & Phrases"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "source": [
    "# nest_asyncio.apply()\n",
    "# __import__('IPython').embed()\n",
    "\n",
    "c = configparser.ConfigParser()\n",
    "c.read('config.ini')\n",
    "\n",
    "host = c['database']['host']\n",
    "username = c['database']['user']\n",
    "password = c['database']['password']\n",
    "db = c['database']['database']\n",
    "\n",
    "news_api_key = c['newsAuth']['api_key']\n",
    "tiktok_sv_id = c['tiktokAuth']['s_v_web_id']\n",
    "tiktok_tt_id = c['tiktokAuth']['tt_webid']\n",
    "\n",
    "tiktok_auth = {\n",
    "    \"s_v_web_id\": tiktok_sv_id,  # references variables saved from config file\n",
    "    \"tt_webid\": tiktok_tt_id\n",
    "}\n",
    "\n",
    "# sys.setrecursionlimit(10000)\n",
    "\n",
    "class TikTokAPI(TikTokAPI):\n",
    "\n",
    "    # create db connection for mogrify\n",
    "    # DATABASE = DataBase(host, username, password)\n",
    "    # CXN = DATABASE.create_db_connection(db)\n",
    "\n",
    "    def __init__(self, cookie, logger=logging, api=None):\n",
    "        super(TikTokAPI, self).__init__(cookie) # must increase recurison limit\n",
    "        self.cookie = cookie\n",
    "        self.tiktok_count = 0\n",
    "        self.logger = logging.basicConfig(filename='tiktok.log', filemode='w',\n",
    "                                          format=f'%(asctime)s - %(levelname)s - %(message)s')\n",
    "        #self.tiktok_df = \n",
    "\n",
    "    def getVideosByHashtag(self, hashtags, count=3000):\n",
    "        try:\n",
    "            for hashTag in hashtags:\n",
    "                try:\n",
    "                    hashTag = hashTag.replace(\"#\", \"\")\n",
    "                    hashTag_obj = self.getHashTag(hashTag)\n",
    "                    hashTag_id = hashTag_obj[\"challengeInfo\"][\"challenge\"][\"id\"]\n",
    "                except (KeyError, ReferenceError) as err:\n",
    "                    logging.error(err)\n",
    "                    continue\n",
    "                else:\n",
    "                    url = self.base_url + \"/challenge/item_list/\"\n",
    "                    req_default_params = {\n",
    "                        \"secUid\": \"\",\n",
    "                        \"type\": \"3\",\n",
    "                            \"minCursor\": \"0\",\n",
    "                            \"maxCursor\": \"0\",\n",
    "                            \"shareUid\": \"\",\n",
    "                            \"recType\": \"\"\n",
    "                        }\n",
    "                    params = {\n",
    "                        \"challengeID\": str(hashTag_id),\n",
    "                            \"count\": str(count),\n",
    "                            \"cursor\": \"0\",\n",
    "                        }\n",
    "                    for key, val in req_default_params.items():\n",
    "                            params[key] = val\n",
    "                    for key, val in self.default_params.items():\n",
    "                            params[key] = val\n",
    "                    extra_headers = {\n",
    "                            \"Referer\": \"https://www.tiktok.com/tag/\" + str(hashTag)}\n",
    "                    self.tiktok_count += 1\n",
    "                    toks = self.send_get_request(url, params, extra_headers=extra_headers)\n",
    "                    print(self.tiktok_count)\n",
    "                    self.tiktok_df = pd.DataFrame(toks, columns=[\n",
    "                        'postID', 'createTime', 'userID', 'description', 'musicId', 'soundId', 'tags'])\n",
    "                    return toks\n",
    "        except KeyboardInterrupt as ex:\n",
    "            raise ex\n",
    "        finally:\n",
    "            logging.info(f'This run scraped {self.tiktok_count} TikToks')\n",
    "            return self.tiktok_df\n",
    "            # DATABASE.execute_mogrify(CXN, tiktok_df, )\n",
    "            # return tiktok_df\n",
    "            # TODO mogrify into database\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "source": [
    "\n",
    "api = TikTokAPI(cookie=tiktok_auth)\n",
    "#api.tiktok_df =\n",
    "tiktoks = news.all_news_df['keywords'].map(api.getVideosByHashtag)\n"
   ],
   "outputs": [
    {
     "output_type": "error",
     "ename": "AttributeError",
     "evalue": "'TikTokAPI' object has no attribute 'cookie'",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-44-8484055254c1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mapi\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTikTokAPI\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcookie\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtiktok_auth\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;31m#api.tiktok_df =\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mtiktoks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnews\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mall_news_df\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'keywords'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mapi\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetVideosByHashtag\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-43-f7f295d75a5d>\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, cookie, logger, api)\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcookie\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogger\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlogging\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mapi\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 30\u001b[0;31m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mTikTokAPI\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcookie\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# must increase recurison limit\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     31\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcookie\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcookie\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtiktok_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-41-1735e25ff33e>\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, cookie, logger, api)\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcookie\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogger\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlogging\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mapi\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 30\u001b[0;31m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mTikTokAPI\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcookie\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# must increase recurison limit\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     31\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcookie\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcookie\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtiktok_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'TikTokAPI' object has no attribute 'cookie'"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "print(tiktok_df)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 7. Add Late-Arriving Dimensions/Data\n",
    "### *data corresponding to 3 days before news hit"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# tweet search instead of stream\n",
    "keywords = news.get_all_news()\n",
    "tw = t.tweet_search(keywords)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 7. Tally Up\n",
    "### Partition total mentions by day"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Add to database"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# mogrify stream\n",
    "# postgres_db.execute_mogrify(connection, filtered_stream, 'stream_tweets')\n",
    "# mogrify batch tweets\n",
    "# postgres_db.execute_mogrify(connection, tw, 'batch_tweets')\n",
    "# # execute mogrify - insert news df into database\n",
    "# postgres_db.execute_mogrify(connection, news.all_news_df, 'articles')\n",
    "# mogrify \n",
    "postgres_db.execute_mogrify(connection, api.tiktok_df, 'tiktoks')\n",
    "\n",
    "# TODO group by event? "
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 9. Plot & Analyze\n",
    "- On which platform (Twitter or TikTok) do folks engage with politics the most?\n",
    "- Where in the US is engagement the highest?\n",
    "- Which political events seem to cause the most reaction among youth?"
   ],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.9.6",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.9.6 64-bit ('data_eng_projects': conda)"
  },
  "interpreter": {
   "hash": "292a7ef3bed24448555716cf5e78212297c16b0757a55b8597bb1f802fb134d0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}