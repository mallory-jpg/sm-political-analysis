{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# TikTok Political Analysis\n",
    "## ~Objectives\n",
    "### Problems & Questions\n",
    "__How can we better develop educational materials to meet kids where they are?__\n",
    "- is it worth it to spend money to advertise to youth for political campaigns - are they engaging with current events?\n",
    "- what are kids talking about & why? What does our education system tell them and not tell them\n",
    "\n",
    "### Goals\n",
    "- understanding how age/youth impacts political indoctrination\n",
    "- understanding social impacts of political events\n",
    "- to understand colloquial knowledge of political concepts\n",
    "\n",
    "## ~Scope\n",
    "- daily batch updates\n",
    "- parsed news events triggers TikTok & twitter queries \n",
    "- topic counts 3 days before event cumulatively added to event day & 3 days following event\n",
    "- see trend lines of engagement on Twitter & TikTok\n",
    "\n",
    "### Overview:\n",
    "- Use NewsAPI to find top news by day\n",
    "- Parse news story title & article into individual words/phrases\n",
    "- Count most important individual words & phrases\n",
    "- Use top 3 most important words & phrases to create rules for searching the Twitter API\n",
    "- Count number of tweets mentioning words & phrases filtered by rules\n",
    "- Use top 3 words & phrases to find similar tags on TikTok API\n",
    "- Count number of TikTok challenges/tags/captions with top words & phrases\n",
    "\n",
    "## ~Extras\n",
    "- age inference of users\n",
    "- sentiment analysis (TextBlob)"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "---"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 1. Install Dependencies & Import Modules\n",
    "- Newsapi-python: pip install newsapi-python\n",
    "- Tweepy: pip install tweepy"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "import requests\n",
    "from datetime import date, timedelta\n",
    "from bs4 import BeautifulSoup\n",
    "import zlib\n",
    "import TikTokApi"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "import numpy as np\n",
    "from sqlalchemy import create_engine\n",
    "import logging\n",
    "import psycopg2\n",
    "from psycopg2 import Error\n",
    "import configparser\n",
    "from timer import Timer\n",
    "\n",
    "c = configparser.ConfigParser()\n",
    "c.read('config.ini')\n",
    "\n",
    "# config credentials\n",
    "host = c['database']['host']\n",
    "username = c['database']['user']\n",
    "password = c['database']['password']\n",
    "db = c['database']['database']\n",
    "\n",
    "\n",
    "c = configparser.ConfigParser()\n",
    "c.read('config.ini')\n",
    "\n",
    "# references .config credentials\n",
    "host = c['database']['host']\n",
    "username = c['database']['user']\n",
    "password = c['database']['password']\n",
    "db = c['database']['database']\n",
    "\n",
    "\n",
    "class DataBase():\n",
    "    def __init__(self, host_name, user_name, user_password):\n",
    "        self.host_name = host_name\n",
    "        self.user_name = user_name\n",
    "        self.user_password = user_password\n",
    "\n",
    "    def create_server_connection(self):\n",
    "        self.connection = None\n",
    "        try:\n",
    "            self.connection = psycopg2.connect(\n",
    "                host=self.host_name,\n",
    "                user=self.user_name,\n",
    "                password=self.user_password\n",
    "            )\n",
    "            logging.info(\"Database connection successful\")\n",
    "        except Error as err:\n",
    "            logging.error(f\"Error: '{err}'\")\n",
    "\n",
    "        return self.connection\n",
    "\n",
    "\n",
    "    def create_database(self, connection, query):\n",
    "            self.connection = connection\n",
    "            cursor = connection.cursor()\n",
    "            try:\n",
    "                cursor.execute(query)\n",
    "                logging.info(\"Database created successfully\")\n",
    "            except Error as err:\n",
    "                logging.error(f\"Error: '{err}'\")\n",
    "\n",
    "\n",
    "    def create_db_connection(self, db_name):\n",
    "            self.db_name = db_name\n",
    "            self.connection = None\n",
    "            try:\n",
    "                connection = psycopg2.connect(\n",
    "                    host=self.host_name,\n",
    "                    user=self.user_name,\n",
    "                    password=self.user_password,\n",
    "                    database=self.db_name\n",
    "                )\n",
    "                # cursor = connection.cursor()\n",
    "                logging.info(\"PostgreSQL Database connection successful\")\n",
    "            except Error as err:\n",
    "                logging.error(f\"Error: '{err}'\")\n",
    "\n",
    "            return self.connection\n",
    "\n",
    "    @Timer(name='Query Execution')\n",
    "    def execute_query(self, connection, query):\n",
    "            self.connection = connection\n",
    "            cursor = self.connection.cursor()\n",
    "            try:\n",
    "                cursor.execute(query)\n",
    "                self.connection.commit()\n",
    "                logging.info(\"Query successful\")\n",
    "            except Error as err:\n",
    "                print(f\"Error: '{err}'\")\n",
    "    \n",
    "    def read_query(self, connection, query):\n",
    "        self.connection = connection\n",
    "        cursor = self.connection.cursor()\n",
    "        result = None\n",
    "        try:\n",
    "            cursor.execute(query)\n",
    "            result = cursor.fetchall()\n",
    "            return result\n",
    "        except Error as err:\n",
    "            logging.error(f\"Error: '{err}'\")\n",
    "\n",
    "    @Timer(name='Mogrify')\n",
    "    def execute_mogrify(self, conn, df, table):\n",
    "        \"\"\"\n",
    "        Using cursor.mogrify() to build the bulk insert query\n",
    "        then cursor.execute() to execute the query\n",
    "        \"\"\"\n",
    "        self.connection = conn\n",
    "        # Create a list of tupples from the dataframe values\n",
    "        tuples = [tuple(x) for x in df.to_numpy()]\n",
    "    \n",
    "        # Comma-separated dataframe columns\n",
    "        cols = ','.join(list(df.columns))\n",
    "    \n",
    "        # SQL query to execute\n",
    "        cursor = conn.cursor()\n",
    "        values = [cursor.mogrify(\"(%s,%s,%s,%s)\", tup).decode('utf8')\n",
    "                for tup in tuples]\n",
    "        # if not publishedAt, delete record\n",
    "        query = \"INSERT INTO %s(%s) VALUES\" % (table, cols) + \",\".join(values)\n",
    "\n",
    "        try:\n",
    "            cursor.execute(query, tuples)\n",
    "            conn.commit()\n",
    "        except (Exception, psycopg2.DatabaseError) as error:\n",
    "            logging.error(\"Error: %s\" % error)\n",
    "            print(\"Error: %s\" % error)\n",
    "            conn.rollback()\n",
    "            cursor.close()\n",
    "            conn.close()\n",
    "            return 1\n",
    "        logging.info(\"execute_mogrify() done\")\n",
    "        cursor.close()\n",
    "        conn.close()\n",
    "\n",
    "# DDL queries\n",
    "create_database_query = \"\"\"\n",
    "        CREATE DATABASE IF NOT EXISTS sm_news; \n",
    "    \"\"\"\n",
    "    # create necessary tables\n",
    "    # keyWords VARCHAR\n",
    "create_article_table = \"\"\"\n",
    "    CREATE TABLE IF NOT EXISTS articles (\n",
    "        publishedAt DATE,\n",
    "        title VARCHAR PRIMARY KEY,\n",
    "        author VARCHAR,\n",
    "        url TEXT\n",
    "        );\n",
    "    \"\"\"    \n",
    "create_article_table_index = \"\"\"\n",
    "    CREATE INDEX index \n",
    "        ON articles(publishedAt, \n",
    "            title\n",
    "        );\n",
    "    \"\"\"\n",
    "create_article_text_table = \"\"\"\n",
    "    CREATE TABLE IF NOT EXISTS article_text (\n",
    "        title VARCHAR PRIMARY KEY REFERENCES articles (title),\n",
    "        text\n",
    "        );\n",
    "    \"\"\"\n",
    "create_article_text_table_index = \"\"\"\n",
    "    CREATE INDEX index \n",
    "        ON article_text(publishedAt, title\n",
    "        );\n",
    "    \"\"\"\n",
    "    # CREATE INDEX index ON articles(publishedAt);\n",
    "create_political_event_table = \"\"\"\n",
    "    CREATE TABLE IF NOT EXISTS event (\n",
    "        eventID VARCHAR PRIMARY KEY,\n",
    "        startDate DATE,\n",
    "        name VARCHAR NOT NULL,\n",
    "        description VARCHAR NOT NULL,\n",
    "        keyWords VARCHAR\n",
    "        );\n",
    "    \"\"\"\n",
    "create_tweets_table = \"\"\"\n",
    "    CREATE TABLE IF NOT EXISTS tweets (\n",
    "        tweet_id INT PRIMARY KEY,\n",
    "        publishedAt DATE NOT NULL,\n",
    "        userID INT NOT NULL,\n",
    "        tweet VARCHAR NOT NULL,\n",
    "        location VARCHAR NOT NULL, \n",
    "        tags VARCHAR NOT NULL\n",
    "        );\n",
    "    \"\"\"\n",
    "create_tiktoks_table = \"\"\"\n",
    "    CREATE TABLE IF NOT EXISTS tiktoks (\n",
    "        postID INT PRIMARY KEY,\n",
    "        createTime DATE NOT NULL,\n",
    "        description VARCHAR NOT NULL,\n",
    "        musicID VARCHAR NOT NULL,\n",
    "        tags VARCHAR NOT NULL,\n",
    "        FOREIGN KEY(songID) REFERENCES tiktok_music(songID),\n",
    "        FOREIGN KEY(soundID) REFERENCES tiktok_sounds(soundID),\n",
    "        FOREIGN KEY(userID) REFERENCES users(userID)\n",
    "        );\n",
    "    \"\"\"\n",
    "create_tiktok_sounds_table = \"\"\"\n",
    "    CREATE TABLE IF NOT EXISTS tiktok_sounds (\n",
    "        soundID INT PRIMARY KEY,\n",
    "        soundTitle VARCHAR,\n",
    "        isOriginal BOOLEAN\n",
    "        );\n",
    "    \"\"\"\n",
    "create_tiktok_music_table = \"\"\"\n",
    "    CREATE TABLE IF NOT EXISTS tiktok_music (\n",
    "        songID INT PRIMARY KEY,\n",
    "        songTitle VARCHAR NOT NULL\n",
    "        );\n",
    "    \"\"\"\n",
    "create_tiktok_stats_table = \"\"\"\n",
    "    CREATE TABLE IF NOT EXISTS tiktok_stats (\n",
    "        FOREIGN KEY(postID) REFERENCES tiktoks(postID),\n",
    "        shareCount INT,\n",
    "        commentCount INT,\n",
    "        playCount INT,\n",
    "        diggCount INT\n",
    "        );\n",
    "    \"\"\"\n",
    "\n",
    "create_tiktok_tags_table = \"\"\"\n",
    "    CREATE TABLE IF NOT EXISTS tiktok_tags (\n",
    "        tagID INT PRIMARY KEY,\n",
    "        tag_name VARCHAR NOT NULL \n",
    "        );\n",
    "    \"\"\"\n",
    "create_users_table = \"\"\"\n",
    "    CREATE TABLE IF NOT EXISTS users (\n",
    "        userID INT PRIMARY KEY,\n",
    "        username VARCHAR NOT NULL,\n",
    "        user_bio VARCHAR NOT NULL\n",
    "        );\n",
    "    \"\"\"\n",
    "delete_bad_data = \"\"\"\n",
    "    DELETE FROM articles\n",
    "        WHERE publishedAt IS NULL;\n",
    "    \"\"\"\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 2. Find Top News by Day"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "\"\"\"\"Extract keywords from  news articles to use as search values for TikTok & Twitter posts relating to the political event of interest. \"\"\"\n",
    "\n",
    "from numpy import datetime64\n",
    "from database import *\n",
    "import logging\n",
    "from datetime import date, datetime, timedelta\n",
    "import pandas as pd\n",
    "import json\n",
    "import requests\n",
    "from nltk import tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from operator import itemgetter\n",
    "import math\n",
    "from bs4 import BeautifulSoup\n",
    "from django.db import DatabaseError\n",
    "\n",
    "# best_words = []\n",
    "# word_df = {}\n",
    "\n",
    "news_api_key = c['newsAuth']['api_key']\n",
    "tiktok_id = c['tiktokAuth']['s_v_web_id']\n",
    "twitter_api_key = c['twitterAuth']['api_key']\n",
    "\n",
    "class News():\n",
    "    def __init__(self, api_key, logger=logging):\n",
    "        self.api_key = api_key\n",
    "        self.logger = logging.basicConfig(filename='news.log', filemode='w',\n",
    "                    format=f'%(asctime)s - %(levelname)s - %(message)s')\n",
    "\n",
    "    def request_pop_news(self, params={\n",
    "        'q': ['politics' or 'political' or 'law' or 'legal' or 'policy'],\n",
    "        'from': {date.today() - timedelta(days=3)},\n",
    "        'to': {date.today},\n",
    "        'language': 'en',\n",
    "        'sort_by': 'popularity'\n",
    "    }):\n",
    "        self.pop_news = []\n",
    "        self.params = params\n",
    "\n",
    "        headers = {\n",
    "            'X-Api-Key': self.api_key\n",
    "        }\n",
    "\n",
    "        url = 'https://newsapi.org/v2/everything'\n",
    "\n",
    "        # response as JSON dict\n",
    "        response = requests.get(url, params=self.params, headers=headers).json()\n",
    "\n",
    "        with open('pop_news.json', 'w') as f:\n",
    "            # write results to JSON file\n",
    "            json.dump(response, f)\n",
    "\n",
    "        with open('pop_news.json', 'r') as file:\n",
    "            # create Python list object from JSON\n",
    "            pop_news_json = file.read().split(\"\\n\")\n",
    "\n",
    "            for story in pop_news_json:\n",
    "                pop_obj = json.loads(story)\n",
    "\n",
    "                if 'title' in pop_obj:\n",
    "                    pop_obj['title'] = pop_obj['articles']['title']\n",
    "                if 'author' in pop_obj:\n",
    "                    pop_obj['author'] = pop_obj['articles']['author']\n",
    "                if 'url' in pop_obj:\n",
    "                    pop_obj['url'] = pop_obj['articles']['url']\n",
    "                if 'publishedAt' in pop_obj:\n",
    "                    pop_obj['publishedAt'] = pop_obj['articles']['publishedAt']\n",
    "\n",
    "                # add info to pop_news dict\n",
    "                self.pop_news.append(pop_obj)\n",
    "        \n",
    "        # load returned results into Pandas dataframe\n",
    "        # flatten data to dataframe\n",
    "        pop_news = pd.json_normalize(self.pop_news, record_path=['articles'])\n",
    "        self.pop_news_df = pd.DataFrame(\n",
    "                pop_news, columns=['title', 'author', 'url', 'publishedAt'])\n",
    "        self.pop_news_df.dropna(axis=0, how='any')\n",
    "\n",
    "        return self.pop_news_df\n",
    "\n",
    "    def get_top_headlines(self, params={\n",
    "        \"language\": \"en\",\n",
    "        \"country\": \"us\"\n",
    "    }):\n",
    "\n",
    "        self.top_headlines = []\n",
    "        self.params = params\n",
    "\n",
    "        headers = {\n",
    "            \"X-Api-Key\": self.api_key\n",
    "        }\n",
    "        url = \"https://newsapi.org/v2/top-headlines\"\n",
    "\n",
    "        response = requests.get(\n",
    "            url, params=self.params, headers=headers).json()  # response JSON dict\n",
    "\n",
    "        with open(\"top_headlines.json\", \"w\") as f:\n",
    "            # write results to JSON file\n",
    "            json.dump(response, f)\n",
    "\n",
    "        with open(\"top_headlines.json\", \"r\") as file:\n",
    "            # create Python object from JSON\n",
    "            top_headlines_json = file.read().split(\"\\n\")\n",
    "\n",
    "            for story in top_headlines_json:\n",
    "                story_obj = json.loads(story)\n",
    "\n",
    "                if 'title' in story_obj:\n",
    "                    story_obj[\"title\"] = story_obj[\"articles\"][\"title\"]\n",
    "                if 'author' in story_obj:\n",
    "                    story_obj[\"author\"] = story_obj[\"articles\"][\"author\"]\n",
    "                if 'url' in story_obj:\n",
    "                    story_obj[\"url\"] = story_obj[\"articles\"][\"url\"]\n",
    "                if 'publishedAt' in story_obj:\n",
    "                    story_obj[\"publishedAt\"] = story_obj[\"articles\"][\"publishedAt\"]\n",
    "\n",
    "                # add info to top_headlines list/dict\n",
    "                self.top_headlines.append(story_obj)\n",
    "            \n",
    "        # flatten data to dataframe\n",
    "        top_headlines = pd.json_normalize(self.top_headlines, record_path=['articles'])\n",
    "        self.top_headlines_df = pd.DataFrame(\n",
    "                top_headlines, columns=[\"title\", \"author\", \"url\", \"publishedAt\"])\n",
    "        self.top_headlines_df.dropna(axis=0, how='any')\n",
    "\n",
    "        return self.top_headlines_df\n",
    "\n",
    "    # put all news together\n",
    "    def all_news(self):\n",
    "        # call class functions\n",
    "        top_headlines = self.get_top_headlines()\n",
    "        pop_news = self.request_pop_news()\n",
    "\n",
    "        # combine result dfs\n",
    "        self.all_news_df = pd.concat([top_headlines, pop_news])\n",
    "\n",
    "        # convert to datetime\n",
    "        self.all_news_df['publishedAt'] = self.all_news_df['publishedAt'].apply(\n",
    "            lambda row: datetime.strptime(row, '%Y-%m-%d %H:%M:%S'), # TODO check formatting\n",
    "            axis=0)\n",
    "\n",
    "        return self.all_news_df\n",
    "\n",
    "    \n",
    "    def article_text(self, url):\n",
    "        \"\"\"Get news article text using Requests and BeautifulSoup\"\"\"\n",
    "        #create dataframe to store text\n",
    "        self.article_text_df = pd.DataFrame({'index': '',\n",
    "                                'title': '',\n",
    "                                'text': '',\n",
    "                                'keyword1': '',\n",
    "                                'keyword2': '',\n",
    "                                'keyword3': ''\n",
    "                                })\n",
    "\n",
    "        r = requests.get(url)\n",
    "        html = r.text\n",
    "        soup = BeautifulSoup(html)\n",
    "        text = soup.get_text()\n",
    "\n",
    "        return text\n",
    "\n",
    "    def keyword_extraction(self, text):\n",
    "        \"\"\"Determine weight of important words in articles and add to articles_text table\n",
    "        using TF-IDF ranking\"\"\"\n",
    "\n",
    "        # make sure text is in string format for parsing\n",
    "        text = str(text)\n",
    "        stop_words = set(stopwords.words('english'))\n",
    "\n",
    "        # find total words in document for calculating Term Frequency (TF)\n",
    "        total_words = text.split()\n",
    "        total_word_length = len(total_words)\n",
    "\n",
    "        # find total number of sentences for calculating Inverse Document Frequency\n",
    "        total_sentences = tokenize.sent_tokenize(text)\n",
    "        total_sent_len = len(total_sentences)\n",
    "\n",
    "        # calculate TF for each word\n",
    "        self.tf_score = {}\n",
    "        for each_word in total_words:\n",
    "            each_word = each_word.replace('.', '')\n",
    "            if each_word not in stop_words:\n",
    "                if each_word in self.tf_score:\n",
    "                    self.tf_score[each_word] += 1\n",
    "                else:\n",
    "                    self.tf_score[each_word] = 1\n",
    "\n",
    "        # Divide by total_word_length for each dictionary element\n",
    "        self.tf_score.update((x, y/int(total_word_length))\n",
    "                        for x, y in self.tf_score.items())  # test - ZeroError\n",
    "\n",
    "        #calculate IDF for each word\n",
    "        self.idf_score = {}\n",
    "        for each_word in total_words:\n",
    "            each_word = each_word.replace('.', '')\n",
    "            if each_word not in stop_words:\n",
    "                if each_word in self.idf_score:\n",
    "                    self.idf_score[each_word] = self.check_sent(each_word, total_sentences)\n",
    "                else:\n",
    "                    self.idf_score[each_word] = 1\n",
    "\n",
    "        # Performing a log and divide\n",
    "        self.idf_score.update((x, math.log(int(total_sent_len)/y))\n",
    "                        for x, y in self.idf_score.items())\n",
    "\n",
    "        # Calculate IDF * TF for each word\n",
    "        self.tf_idf_score = {key: self.tf_score[key] *\n",
    "                        self.idf_score.get(key, 0) for key in self.tf_score.keys()}\n",
    "\n",
    "        return self.tf_idf_score\n",
    "\n",
    "    def check_sent(self, word, sentences):\n",
    "        \"\"\"Check if word is present in sentence list for calculating IDF (Inverse Document Frequency)\"\"\"\n",
    "        final = [all([w in x for w in word]) for x in sentences]\n",
    "        sent_len = [sentences[i] for i in range(0, len(final)) if final[i]]\n",
    "    \n",
    "        return int(len(sent_len))\n",
    "\n",
    "    def get_top_n(self, dict_elem, n):\n",
    "        \"\"\"Calculate most important keywords in text of interest\"\"\"\n",
    "        result = dict(sorted(dict_elem.items(),\n",
    "                    key=itemgetter(1), reverse=True)[:n])\n",
    "        return result\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 3. Parse Titles & Articles"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 4. Count Important Words & Phrases"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 5. Search Twitter API\n",
    "## Using Important Words & Phrases"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "import get_news\n",
    "\n",
    "import tweepy  # python package for accessing Tweet streaming API\n",
    "from tweepy import API\n",
    "from tweepy import Stream\n",
    "import json\n",
    "import logging\n",
    "import pandas as pd\n",
    "import configparser\n",
    "import requests\n",
    "from datetime import date, timedelta\n",
    "import urllib.parse\n",
    "\n",
    "twitter_config = configparser.ConfigParser()\n",
    "twitter_config.read('config.ini')\n",
    "\n",
    "access_token = twitter_config['twitterAuth']['access_token']\n",
    "access_token_secret = twitter_config['twitterAuth']['access_token_secret']\n",
    "consumer_key = twitter_config['twitterAuth']['consumer_key']\n",
    "consumer_secret = twitter_config['twitterAuth']['consumer_secret']\n",
    "\n",
    "\n",
    "class Tweets():\n",
    "    \n",
    "    def __init__(self, consumer_key, consumer_secret, access_token, access_token_secret, logger=logging):\n",
    "        #self.logger = logging.basicConfig(filename='tweets.log', filemode='w',\n",
    "                                         #format=f'%(asctime)s - %(levelname)s - %(message)s')\n",
    "        self.consumer_key = consumer_key\n",
    "        self.consumer_secret = consumer_secret\n",
    "        self.access_token = access_token\n",
    "        self.access_token_secret = access_token_secret\n",
    "        self.logger = logging.getLogger(__name__)\n",
    "\n",
    "    def tweepy_auth(self):\n",
    "\n",
    "        self.auth = tweepy.OAuthHandler(self.consumer_key, self.consumer_secret)\n",
    "        self.auth.set_access_token(self.access_token, self.access_token_secret)\n",
    "\n",
    "        # create API object\n",
    "        self.api = API(self.auth, wait_on_rate_limit=True, wait_on_rate_limit_notify=True)\n",
    "\n",
    "        try:\n",
    "            self.api.verify_credentials()\n",
    "        except Exception as e:\n",
    "            self.logger.error(\"Error during Tweepy authentication\")\n",
    "            raise e\n",
    "        \n",
    "        self.logger.info(\"Tweepy API Authenticated\")\n",
    "    \n",
    "    def tweet_search(self, query):\n",
    "        \"\"\"Search for tweets within previous 7 days.\n",
    "            Inputs: \n",
    "                https-encoded query\n",
    "                language\n",
    "                'until' date\n",
    "                geocode (latitude/longitude)\n",
    "            Returns: \n",
    "                Tweet object\n",
    "        \"\"\"\n",
    "        self.tweet_search_list = []\n",
    "        query = urllib.parse.urlencode(query)\n",
    "        # latitude & longitude of Colombus, OH, USA\n",
    "        latitude = '39.9828671'\n",
    "        longitude = '-83.1309131'\n",
    "        # radius of united states\n",
    "        radius = '3881mi'\n",
    "\n",
    "        query_result = tweepy.Cursor(self.api, q=query, lang='en', until={\n",
    "                                     date.today()}, geocode=[latitude, longitude, radius])\n",
    "\n",
    "        for status in tweepy.Cursor(query_result).items():\n",
    "            self.tweet_search_list.append(status)\n",
    "            return self.tweet_search_list\n",
    "\n",
    "        # TODO append tweets to dataframe & return it\n",
    "        self.tweet_search_df = pd.DataFrame(self.tweet_search_list)\n",
    "        return self.tweet_search_df\n",
    "        \n",
    "    def tweet_trends(self):\n",
    "            # returns JSON\n",
    "        # 1 refers to USA WOEID \n",
    "        self.tweet_trends_list = []\n",
    "        result = tweepy.Cursor(self.api.trends_place(1))\n",
    "\n",
    "        for trend in tweepy.Cursor(result).items():\n",
    "            self.tweet_trends_list.append(trend)\n",
    "            return self.tweet_trends_list\n",
    "        \n",
    "        #TODO append to dataframe\n",
    "        self.tweet_trends_df = pd.DataFrame(self.tweet_trends_list)\n",
    "        return self.tweet_trends_df    \n",
    "\n",
    "# define stream listener class\n",
    "class TwitterStreamListener(tweepy.StreamListener):\n",
    "    def __init__(self, api=None):\n",
    "        super(TwitterStreamListener, self).__init__()\n",
    "        self.num_tweets = 0\n",
    "        # self.file = open('tweets.txt', 'w')\n",
    "        # self.db = ''\n",
    "        self.tweet_list = []\n",
    "        # self.file = open(\"tweets.json\", \"w\")\n",
    "\n",
    "    def on_status(self, status):\n",
    "        tweet = status._json\n",
    "\n",
    "        with open(\"tweets.json\", \"w\") as f:\n",
    "            # write tweets to json file\n",
    "            json.dump(tweet, f)\n",
    "        \n",
    "        with open(\"tweets.json\", \"r\") as file:\n",
    "            # create python object from json\n",
    "            tweets_json = file.read().split(\"\\n\")\n",
    "\n",
    "            for tweet in tweets_json:\n",
    "                tweet_obj = json.loads(tweet)\n",
    "\n",
    "                #flatten nested fields\n",
    "                if 'quoted_status' in tweet_obj:\n",
    "                    tweet_obj['quote_tweet'] =tweet_obj['quoted_status']['extended_tweet']['full_text']\n",
    "                if 'user' in tweet_obj:\n",
    "                    tweet_obj['location'] = tweet_obj['user']['location']\n",
    "                # if 'created_at' in tweet_obj:\n",
    "                #     tweet_obj['created_at'] = pd.to_datetime(tweet)\n",
    "                \n",
    "\n",
    "                self.tweet_list.append(status)\n",
    "                self.num_tweets += 1\n",
    "\n",
    "                # flatten data to dataframe\n",
    "                # tweets = pd.json_normalize(self.tweet_list, record_path=['articles'])\n",
    "                self.tweets_df = pd.DataFrame(self.tweet_list, columns=[\"tweet_id\", \"publishedAt\", \"userID\", \"text\", \"location\"])\n",
    "\n",
    "                return self.tweets_df\n",
    "            \n",
    "        if self.num_tweets < 450:  # whatever the max stream rate is for the twitter API Client\n",
    "            return True\n",
    "        else:\n",
    "            return False\n",
    "\n",
    "\n",
    "\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 6. Search TikTok\n",
    "## Using Important Words & Phrases"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 7. Add Late-Arriving Dimensions/Data\n",
    "### *data corresponding to 3 days before news hit"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "from timer import Timer\n",
    "from database import *\n",
    "from get_news import *\n",
    "import configparser\n",
    "\n",
    "# configure ConfigParser\n",
    "c = configparser.ConfigParser()\n",
    "c.read('config.ini')\n",
    "\n",
    "# references .config credentials\n",
    "host = c['database']['host']\n",
    "username = c['database']['user']\n",
    "password = c['database']['password']\n",
    "db = c['database']['database']\n",
    "\n",
    "news_api_key = c['newsAuth']['api_key']\n",
    "tiktok_id = c['tiktokAuth']['s_v_web_id']\n",
    "twitter_api_key = c['twitterAuth']['api_key']\n",
    "\n",
    "# instantiate DataBase class using .config files\n",
    "postgres_db = DataBase(host, username, password)\n",
    "\n",
    "# instantiate News class\n",
    "news = News(news_api_key)\n",
    "\n",
    "# connect to server\n",
    "postgres_server = postgres_db.create_server_connection()\n",
    "\n",
    "# connect to social media news db\n",
    "connection = postgres_db.create_db_connection(db)\n",
    "\n",
    "# execute defined queries to create db tables if needed\n",
    "\n",
    "\n",
    "try:\n",
    "    postgres_db.execute_query(connection, create_article_table)\n",
    "    postgres_db.execute_query(connection, create_article_text_table)\n",
    "    postgres_db.execute_query(connection, create_tweets_table)\n",
    "    postgres_db.execute_query(connection, create_political_event_table)\n",
    "\n",
    "    postgres_db.execute_query(connection, create_tiktok_sounds_table)\n",
    "    postgres_db.execute_query(connection, create_tiktok_music_table)\n",
    "    postgres_db.execute_query(\n",
    "    connection, create_tiktok_stats_table)  # not running?\n",
    "    postgres_db.execute_query(connection, create_tiktok_tags_table)\n",
    "    postgres_db.execute_query(connection, create_tiktoks_table)\n",
    "except (DatabaseError, ConnectionError) as e:\n",
    "    logging.error({e}, 'Check SQL queries')\n",
    "\n",
    "# news.request_pop_news()\n",
    "# news.get_top_headlines()\n",
    "\n",
    "# apply get_text function using urls from all_news df\n",
    "url_text = news.all_news_df['url'].apply(\n",
    "        lambda row: news.article_text(news.all_news_df['url']),\n",
    "        axis=1)\n",
    "# put url_text into df\n",
    "news.article_text_df['text'] = url_text\n",
    "\n",
    "# get keywords from article text\n",
    "\n",
    "# article_text_df['keys'] = keyword_extraction(article_text)\n",
    "# get top 3 words of significance\n",
    "keywords = news.article_text_df['keywords'].apply(\n",
    "    lambda row: news.get_top_n(news.tf_idf_score, 3)\n",
    ")\n",
    "\n",
    "# TODO test get_news & find order of key:value pairs\n",
    "news.article_text_df['keyword1'] = keywords[1]\n",
    "news.article_text_df['keyword2'] = keywords[2]\n",
    "news.article_text_df['keyword3'] = keywords[3]\n",
    "\n",
    "\n",
    "# execute mogrify - insert news into database\n",
    "postgres_db.execute_mogrify(connection, news.all_news_df, 'articles')\n",
    "\n",
    "# append text and keys to database\n",
    "postgres_db.execute_mogrify(connection, news.article_text_df, 'article_text')\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 8. Tally Up\n",
    "### Partition counts by day"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 9. Plot & Analyze\n",
    "- On which platform (Twitter or TikTok) do folks engage with politics the most?\n",
    "- Where in the US is engagement the highest?\n",
    "- Which political events seem to cause the most reaction among youth?"
   ],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.9.4",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.9.4 64-bit (conda)"
  },
  "interpreter": {
   "hash": "292a7ef3bed24448555716cf5e78212297c16b0757a55b8597bb1f802fb134d0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}