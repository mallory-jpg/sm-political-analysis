{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# TikTok Political Analysis\n",
    "## ~Objectives\n",
    "### Problems & Questions\n",
    "__How can we better develop educational materials to meet kids where they are?__\n",
    "- is it worth it to spend money to advertise to youth for political campaigns - are they engaging with current events?\n",
    "- what are kids talking about & why? What does our education system tell them and not tell them\n",
    "\n",
    "### Goals\n",
    "- understanding how age/youth impacts political indoctrination\n",
    "- understanding social impacts of political events\n",
    "- to understand colloquial knowledge of political concepts\n",
    "\n",
    "## ~Scope\n",
    "- daily batch updates\n",
    "- parsed news events triggers TikTok & twitter queries \n",
    "- topic counts 3 days before event cumulatively added to event day & 3 days following event\n",
    "- see trend lines of engagement on Twitter & TikTok\n",
    "\n",
    "### Overview:\n",
    "- Use NewsAPI to find top news by day\n",
    "- Parse news story title & article into individual words/phrases\n",
    "- Count most important individual words & phrases\n",
    "- Use top 3 most important words & phrases to create rules for searching the Twitter API\n",
    "- Count number of tweets mentioning words & phrases filtered by rules\n",
    "- Use top 3 words & phrases to find similar tags on TikTok API\n",
    "- Count number of TikTok challenges/tags/captions with top words & phrases\n",
    "\n",
    "## ~Extras\n",
    "- age inference of users\n",
    "- sentiment analysis (TextBlob)"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "---"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 1. Install Dependencies & Import Modules\n",
    "- Newsapi-python: pip install newsapi-python\n",
    "- Tweepy (install without virtual environment): pip install tweepy\n",
    "- playwright: pip install playwright\n",
    "                playwright install\n",
    "- TikTokApi (install without virtual environment): pip install TikTokApi --upgrade\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "import requests\n",
    "from datetime import date, timedelta\n",
    "from bs4 import BeautifulSoup\n",
    "import numpy as np\n",
    "import logging\n",
    "import configparser\n",
    "from timer import Timer\n",
    "from numpy import datetime64\n",
    "from datetime import date, datetime, timedelta\n",
    "from nltk import tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from operator import itemgetter\n",
    "import math\n",
    "import tweepy  # python package for accessing Tweet streaming API\n",
    "from tweepy import API\n",
    "from tweepy import Stream\n",
    "import urllib.parse\n",
    "from TikTokApi import TikTokApi\n",
    "from selenium import webdriver\n",
    "import psycopg2 # alts: SQLalchemy - warning: not as simple\n",
    "from psycopg2 import Error\n",
    "import re\n",
    "import sys\n",
    "import geocoder\n",
    "from helper_functions import *"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Configure using config.ini file"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "c = configparser.ConfigParser()\n",
    "c.read('config.ini')\n",
    "\n",
    "# config credentials\n",
    "host = c['database']['host']\n",
    "username = c['database']['user']\n",
    "password = c['database']['password']\n",
    "db = c['database']['database']\n",
    "\n",
    "news_api_key = c['newsAuth']['api_key']\n",
    "tiktok_id = c['tiktokAuth']['s_v_web_id']\n",
    "twitter_api_key = c['twitterAuth']['api_key']\n",
    "\n",
    "access_token = c['twitterAuth']['access_token']\n",
    "access_token_secret = c['twitterAuth']['access_token_secret']\n",
    "consumer_key = c['twitterAuth']['consumer_key']\n",
    "consumer_secret = c['twitterAuth']['consumer_secret']\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "create Database class"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "class DataBase():\n",
    "    def __init__(self, host_name, user_name, user_password):\n",
    "        self.host_name = host_name\n",
    "        self.user_name = user_name\n",
    "        self.user_password = user_password\n",
    "\n",
    "    def create_server_connection(self):\n",
    "        self.connection = None\n",
    "        try:\n",
    "            self.connection = psycopg2.connect(\n",
    "                host=self.host_name,\n",
    "                user=self.user_name,\n",
    "                password=self.user_password\n",
    "            )\n",
    "            logging.info(\"Database connection successful\")\n",
    "        except Error as err:\n",
    "            logging.error(f\"Error: '{err}'\")\n",
    "\n",
    "        return self.connection\n",
    "\n",
    "\n",
    "    def create_database(self, connection, query):\n",
    "            self.connection = connection\n",
    "            cursor = connection.cursor()\n",
    "            try:\n",
    "                cursor.execute(query)\n",
    "                logging.info(\"Database created successfully\")\n",
    "            except Error as err:\n",
    "                logging.error(f\"Error: '{err}'\")\n",
    "\n",
    "\n",
    "    def create_db_connection(self, db_name):\n",
    "            self.db_name = db_name\n",
    "            self.connection = None\n",
    "            try:\n",
    "                self.connection = psycopg2.connect(\n",
    "                    host=self.host_name,\n",
    "                    user=self.user_name,\n",
    "                    password=self.user_password,\n",
    "                    database=self.db_name\n",
    "                )\n",
    "                # cursor = connection.cursor()\n",
    "                logging.info(\"PostgreSQL Database connection successful\")\n",
    "            except Error as err:\n",
    "                logging.error(f\"Error: '{err}'\")\n",
    "\n",
    "            return self.connection\n",
    "\n",
    "    # @Timer(name='Query Execution') #*TODO fix __enter__ attribute error\n",
    "    def execute_query(self, connection, query):\n",
    "            self.connection = connection\n",
    "            cursor = connection.cursor()\n",
    "            try:\n",
    "                cursor.execute(query)\n",
    "                self.connection.commit()\n",
    "                logging.info(\"Query successful\")\n",
    "            except Error as err:\n",
    "                print(f\"Error: '{err}'\")\n",
    "    \n",
    "    def read_query(self, connection, query):\n",
    "        self.connection = connection\n",
    "        cursor = self.connection.cursor()\n",
    "        result = None\n",
    "        try:\n",
    "            cursor.execute(query)\n",
    "            result = cursor.fetchall()\n",
    "            return result\n",
    "        except Error as err:\n",
    "            logging.error(f\"Error: '{err}'\")\n",
    "    \n",
    "\n",
    "    @Timer(name='Mogrify')\n",
    "    def execute_mogrify(self, conn, df, table):\n",
    "        \"\"\"\n",
    "        Using cursor.mogrify() to build the bulk insert query\n",
    "        then cursor.execute() to execute the query\n",
    "        \"\"\"\n",
    "        self.connection = conn\n",
    "        # Create a list of tupples from the dataframe values\n",
    "        tuples = [tuple(x) for x in df.to_numpy()]\n",
    "    \n",
    "        # Comma-separated dataframe columns\n",
    "        cols = ','.join(list(df.columns))\n",
    "    \n",
    "        # SQL query to execute\n",
    "        cursor = conn.cursor()\n",
    "        values = [cursor.mogrify(\"(%s,%s,%s,%s)\", tup).decode('utf8')\n",
    "                for tup in tuples]\n",
    "        # if not publishedAt, delete record\n",
    "        query = \"INSERT INTO %s(%s) VALUES\" % (table, cols) + \",\".join(values)\n",
    "\n",
    "        try:\n",
    "            cursor.execute(query, tuples)\n",
    "            conn.commit()\n",
    "        except (Exception, psycopg2.DatabaseError) as error:\n",
    "            logging.error(\"Error: %s\" % error)\n",
    "            print(\"Error: %s\" % error)\n",
    "            conn.rollback()\n",
    "            cursor.close()\n",
    "            conn.close()\n",
    "            return 1\n",
    "        logging.info(\"execute_mogrify() done\")\n",
    "        cursor.close()\n",
    "        conn.close()\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Variables for SQL queries"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# create db \n",
    "create_database_query = \"\"\"\n",
    "        CREATE DATABASE IF NOT EXISTS sm_news;\n",
    "    \"\"\"\n",
    "# create necessary tables\n",
    "create_article_table = \"\"\"\n",
    "    CREATE TABLE IF NOT EXISTS articles (\n",
    "        publishedAt DATE,\n",
    "        title VARCHAR PRIMARY KEY,\n",
    "        author VARCHAR,\n",
    "        url TEXT\n",
    "        );\n",
    "    \"\"\"\n",
    "create_article_table_index = \"\"\"\n",
    "    CREATE INDEX index\n",
    "        ON articles(publishedAt,\n",
    "            title\n",
    "        );\n",
    "    \"\"\"\n",
    "create_article_text_table = \"\"\"\n",
    "    CREATE TABLE IF NOT EXISTS article_text (\n",
    "        title VARCHAR PRIMARY KEY,\n",
    "        article_text TEXT\n",
    "        );\n",
    "    \"\"\"\n",
    "create_political_event_table = \"\"\"\n",
    "    CREATE TABLE IF NOT EXISTS event (\n",
    "        eventID ID PRIMARY KEY,\n",
    "        startDate DATE,\n",
    "        name VARCHAR NOT NULL,\n",
    "        description VARCHAR NOT NULL,\n",
    "        keyWords VARCHAR\n",
    "        );\n",
    " \"\"\"\n",
    "create_tweets_table = \"\"\"\n",
    "    CREATE TABLE IF NOT EXISTS tweets (\n",
    "        tweet_id INT PRIMARY KEY,\n",
    "        publishedAt DATE NOT NULL,\n",
    "        userID VARCHAR NOT NULL,\n",
    "        tweet VARCHAR NOT NULL,\n",
    "        location VARCHAR NOT NULL, \n",
    "        tags VARCHAR NOT NULL\n",
    "        );\n",
    "    \"\"\"\n",
    "create_tiktoks_table = \"\"\"\n",
    "    CREATE TABLE IF NOT EXISTS tiktoks (\n",
    "        postID INT PRIMARY KEY,\n",
    "        createTime DATE NOT NULL,\n",
    "        userID INT NOT NULL,\n",
    "        description VARCHAR NOT NULL,\n",
    "        musicID INT NOT NULL,\n",
    "        soundID INT NOT NULL,\n",
    "        tags VARCHAR NOT NULL\n",
    "        );\n",
    "    \"\"\"\n",
    "create_tiktok_sounds_table = \"\"\"\n",
    "    CREATE TABLE IF NOT EXISTS tiktok_sounds (\n",
    "        soundID INT PRIMARY KEY,\n",
    "        soundTitle VARCHAR,\n",
    "        isOriginal BOOLEAN\n",
    "        );\n",
    "    \"\"\"\n",
    "create_tiktok_music_table = \"\"\"\n",
    "    CREATE TABLE IF NOT EXISTS tiktok_music (\n",
    "        songID INT PRIMARY KEY,\n",
    "        songTitle VARCHAR NOT NULL\n",
    "        );\n",
    "    \"\"\"\n",
    "\n",
    "create_tiktok_stats_table = \"\"\"\n",
    "    CREATE TABLE IF NOT EXISTS tiktok_stats (\n",
    "        postID INT PRIMARY KEY,\n",
    "        shareCount INT,\n",
    "        commentCount INT,\n",
    "        playCount INT,\n",
    "        diggCount INT\n",
    "        );\n",
    "    \"\"\"\n",
    "\n",
    "create_tiktok_tags_table = \"\"\"\n",
    "    CREATE TABLE IF NOT EXISTS tiktok_tags (\n",
    "        tagID INT PRIMARY KEY,\n",
    "        tag_name VARCHAR NOT NULL \n",
    "        );\n",
    "    \"\"\"\n",
    "create_users_table = \"\"\"\n",
    "    CREATE TABLE IF NOT EXISTS users (\n",
    "        userID INT PRIMARY KEY,\n",
    "        username VARCHAR NOT NULL,\n",
    "        user_bio VARCHAR NOT NULL\n",
    "        );\n",
    "    \"\"\"\n",
    "delete_bad_data = \"\"\"\n",
    "    DELETE FROM articles\n",
    "        WHERE publishedAt IS NULL;\n",
    "    \"\"\"\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Create Database"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "postgres_db = DataBase(host, username, password)\n",
    "\n",
    "# connect to server\n",
    "postgres_server = postgres_db.create_server_connection()\n",
    "\n",
    "# connect to social media news db\n",
    "connection = postgres_db.create_db_connection(db)\n",
    "\n",
    "# execute defined queries to create db tables if needed\n",
    "try:\n",
    "    postgres_db.execute_query(connection, create_article_table) # TODO fix attribute error __enter__ for Timer wrapper\n",
    "    postgres_db.execute_query(connection, create_article_text_table)\n",
    "    postgres_db.execute_query(connection, create_tweets_table)\n",
    "    postgres_db.execute_query(connection, create_political_event_table)\n",
    "    postgres_db.execute_query(connection, create_users_table)\n",
    "    postgres_db.execute_query(connection, create_tiktok_sounds_table)\n",
    "    postgres_db.execute_query(connection, create_tiktok_music_table)\n",
    "    postgres_db.execute_query(connection, create_tiktok_stats_table)\n",
    "    postgres_db.execute_query(connection, create_tiktok_tags_table)\n",
    "    postgres_db.execute_query(connection, create_tiktoks_table)\n",
    "except (ConnectionError) as e:\n",
    "    logging.error({e}, 'Check SQL create queries')\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# add foreign keys\n",
    "alter_tiktoks_table = \"\"\"\n",
    "    ALTER TABLE tiktoks\n",
    "    ADD FOREIGN KEY(musicID) REFERENCES tiktok_music(songID),\n",
    "    ADD FOREIGN KEY(soundID) REFERENCES tiktok_sounds(soundID),\n",
    "    ADD FOREIGN KEY(userID) REFERENCES users(userID)\n",
    "    ON DELETE SET NULL;\n",
    "\"\"\"\n",
    "alter_tiktok_stats_table = \"\"\"\n",
    "    ALTER TABLE tiktok_stats\n",
    "    ADD FOREIGN KEY(postID) REFERENCES tiktoks(postID)\n",
    "    ON DELETE SET NULL;\n",
    "\"\"\"\n",
    "try:\n",
    "    postgres_db.execute_query(connection, alter_tiktoks_table)\n",
    "    postgres_db.execute_query(connection, alter_tiktok_stats_table)\n",
    "except (ConnectionError) as e:\n",
    "    logging.error({e}, 'Check SQL alteration queries')\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 2. Find Top News by Day"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Create News class"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "class News():\n",
    "    \"\"\"Extract keywords from  news articles to use as search values for TikTok & Twitter posts relating to the political event of interest. \"\"\"\n",
    "\n",
    "    def __init__(self, api_key, logger=logging):\n",
    "        self.api_key = api_key\n",
    "        self.logger = logging.basicConfig(filename='news.log', filemode='w',\n",
    "                    format=f'%(asctime)s - %(levelname)s - %(message)s')\n",
    "\n",
    "    def request_pop_news(self, params={\n",
    "        'q': ['politics' or 'political' or 'law' or 'legal' or 'policy'],\n",
    "        'from': {date.today() - timedelta(days=3)},\n",
    "        'to': {date.today},\n",
    "        'language': 'en',\n",
    "        'sort_by': 'popularity'\n",
    "    }):\n",
    "        pop_news = []\n",
    "        self.params = params\n",
    "\n",
    "        headers = {\n",
    "            'X-Api-Key': self.api_key,\n",
    "            # get_random_ua for Chrome\n",
    "            'user-agent': get_random_ua('Chrome')\n",
    "        }\n",
    "\n",
    "        url = 'https://newsapi.org/v2/everything'\n",
    "\n",
    "        # response as JSON dict\n",
    "        self.response = requests.get(url, params=self.params, headers=headers).json()\n",
    "\n",
    "        with open('pop_news.json', 'w') as f:\n",
    "            # write results to JSON file\n",
    "            json.dump(self.response, f)\n",
    "\n",
    "        with open('pop_news.json', 'r') as file:\n",
    "            # create Python list object from JSON\n",
    "            pop_news_json = file.read().split(\"\\n\")\n",
    "\n",
    "            for story in pop_news_json:\n",
    "                pop_obj = json.loads(story)\n",
    "\n",
    "                if 'title' in pop_obj:\n",
    "                    pop_obj['title'] = pop_obj['articles']['title']\n",
    "                if 'author' in pop_obj:\n",
    "                    pop_obj['author'] = pop_obj['articles']['author']\n",
    "                if 'url' in pop_obj:\n",
    "                    pop_obj['url'] = pop_obj['articles']['url']\n",
    "                if 'publishedAt' in pop_obj:\n",
    "                    pop_obj['publishedAt'] = pop_obj['articles']['publishedAt']\n",
    "\n",
    "                # add info to pop_news dict\n",
    "                pop_news.append(pop_obj)\n",
    "\n",
    "        return pop_news\n",
    "\n",
    "    def get_top_headlines(self, params={\n",
    "        \"language\": \"en\",\n",
    "        \"country\": \"us\"\n",
    "    }):\n",
    "\n",
    "        top_headlines = []\n",
    "        self.params = params\n",
    "\n",
    "        headers = {\n",
    "            \"X-Api-Key\": news_api_key,\n",
    "            \"user-agent\": get_random_ua('Chrome')\n",
    "        }\n",
    "        url = \"https://newsapi.org/v2/top-headlines\"\n",
    "\n",
    "        self.response = requests.get(\n",
    "            url, params=self.params, headers=headers).json()  # response JSON dict\n",
    "\n",
    "        with open(\"top_headlines.json\", \"w\") as f:\n",
    "            # write results to JSON file\n",
    "            json.dump(self.response, f)\n",
    "\n",
    "        with open(\"top_headlines.json\", \"r\") as file:\n",
    "            # create Python object from JSON\n",
    "            top_headlines_json = file.read().split(\"\\n\")\n",
    "\n",
    "            for story in top_headlines_json:\n",
    "                story_obj = json.loads(story)\n",
    "\n",
    "                if 'title' in story_obj:\n",
    "                    story_obj[\"title\"] = story_obj[\"articles\"][\"title\"]\n",
    "                if 'author' in story_obj:\n",
    "                    story_obj[\"author\"] = story_obj[\"articles\"][\"author\"]\n",
    "                if 'url' in story_obj:\n",
    "                    story_obj[\"url\"] = story_obj[\"articles\"][\"url\"]\n",
    "                if 'publishedAt' in story_obj:\n",
    "                    story_obj[\"publishedAt\"] = story_obj[\"articles\"][\"publishedAt\"]\n",
    "\n",
    "                # add info to top_headlines list/dict\n",
    "                top_headlines.append(story_obj)\n",
    "\n",
    "        return top_headlines\n",
    "\n",
    "    # put all news together\n",
    "    def get_all_news(self):\n",
    "        \"\"\"Combines top headlines and popular news into one Pandas DataFrame.\"\"\"\n",
    "        top_headlines = self.get_top_headlines()\n",
    "        pop_news = self.request_pop_news()\n",
    "\n",
    "        # noramlize nested JSON\n",
    "        pop_news = pd.json_normalize(pop_news, record_path=['articles'])\n",
    "        top_headlines = pd.json_normalize(top_headlines, record_path=['articles'])\n",
    "        all_news = top_headlines.append(pop_news)\n",
    "\n",
    "        # create dataframe from combined news list\n",
    "        self.all_news_df = pd.DataFrame(\n",
    "            all_news, columns=['title', 'author', 'url', 'publishedAt', \"text\"])\n",
    "        self.all_news_df.drop_duplicates()\n",
    "\n",
    "        # convert to datetime\n",
    "        self.all_news_df['publishedAt'] = self.all_news_df['publishedAt'].map(\n",
    "            lambda row: datetime.strptime(str(row), \"%Y-%m-%dT%H:%M:%SZ\") if pd.notnull(row) else row)\n",
    "\n",
    "        # set index to publishing time, inplace to apply to same df instead of copy or view\n",
    "        self.all_news_df.set_index('publishedAt', inplace=True)\n",
    "        \n",
    "        # apply .get_article_text() to text column of df\n",
    "        self.all_news_df[\"text\"] = self.all_news_df[\"url\"].apply(self.get_article_text)\n",
    "        \n",
    "        \n",
    "        return self.all_news_df\n",
    "\n",
    "    \n",
    "    def get_article_text(self, url): \n",
    "        \"\"\"Clean & process news article text to prepare for keyword extraction\"\"\"\n",
    "        \n",
    "        contractions_dict = {\"'s\": \" is\", \"n't\": \" not\", \"'m\": \" am\", \"'ll\": \" will\",\n",
    "                     \"'d\": \" would\", \"'ve\": \" have\", \"'re\": \" are\"}\n",
    "        symbols_list = ['&', '+', '-', '/', '|', '$', '%', ':', '(', ')', '?']\n",
    "        \n",
    "        # request\n",
    "        r = requests.get(url)\n",
    "        html = r.text\n",
    "        soup = BeautifulSoup(html)\n",
    "        a_text = soup.get_text()\n",
    "\n",
    "        # remove newline characters\n",
    "        a_text = a_text.strip()\n",
    "        # split joined words\n",
    "        a_text = \" \".join([s for s in re.split(\"([A-Z][a-z]+[^A-Z]*)\", a_text) if s])\n",
    "        # remove mentions\n",
    "        a_text = re.sub(\"@\\S+\", \" \", a_text)\n",
    "        # remove URLs\n",
    "        a_text = re.sub(\"https*\\S+\", \" \", a_text)\n",
    "        # remove hashtags\n",
    "        a_text = re.sub(\"#\\S+\", \" \", a_text)\n",
    "        # remove unicode characters\n",
    "        a_text = a_text.encode('ascii', 'ignore').decode()\n",
    "        # replace contractions\n",
    "        for key, value in contractions_dict.items():\n",
    "            if key in a_text:\n",
    "                a_text = a_text.replace(key, value)\n",
    "        # remove symbols and punctuation\n",
    "        for i in symbols_list:\n",
    "            if i in a_text:\n",
    "                a_text = a_text.replace(i, '')\n",
    "\n",
    "        # make lowercase\n",
    "        a_text = a_text.lower()\n",
    "        a_text = re.sub(r'\\w*\\d+\\w*', '', a_text)\n",
    "\n",
    "        return a_text\n",
    "\n",
    "    def keyword_extraction(self, text):\n",
    "        \"\"\"Determine weight of important words in articles and add to articles_text table\n",
    "        using TF-IDF ranking\"\"\"\n",
    "\n",
    "        # make sure text is in string format for parsing\n",
    "        text = str(text)\n",
    "        stop_words = set(stopwords.words('english'))\n",
    "\n",
    "        # find total words in document for calculating Term Frequency (TF)\n",
    "        total_words = text.split()\n",
    "        total_word_length = len(total_words)\n",
    "\n",
    "        # find total number of sentences for calculating Inverse Document Frequency\n",
    "        total_sentences = tokenize.sent_tokenize(text)\n",
    "        total_sent_len = len(total_sentences)\n",
    "\n",
    "        # calculate TF for each word\n",
    "        tf_score = {}\n",
    "        for each_word in total_words:\n",
    "            each_word = each_word.replace('.', '')\n",
    "            if each_word not in stop_words and len(each_word) > 3:\n",
    "                if each_word in tf_score:\n",
    "                    tf_score[each_word] += 1\n",
    "                else:\n",
    "                    tf_score[each_word] = 1\n",
    "\n",
    "        # Divide by total_word_length for each dictionary element\n",
    "        tf_score.update((x, y/int(total_word_length))\n",
    "                        for x, y in tf_score.items())  # TODO test - ZeroError\n",
    "\n",
    "        #calculate IDF for each word\n",
    "        idf_score = {}\n",
    "        for each_word in total_words:\n",
    "            each_word = each_word.replace('.', '')\n",
    "            if each_word not in stop_words and len(each_word) > 3:\n",
    "                if each_word in idf_score:\n",
    "                    idf_score[each_word] = self.check_sent(each_word, total_sentences)\n",
    "                else:\n",
    "                    idf_score[each_word] = 1\n",
    "\n",
    "        # Performing a log and divide\n",
    "        idf_score.update((x, math.log(int(total_sent_len)/y))\n",
    "                        for x, y in idf_score.items())\n",
    "\n",
    "        # Calculate IDF * TF for each word\n",
    "        tf_idf_score = {key: tf_score[key] *\n",
    "                        idf_score.get(key, 0) for key in tf_score.keys()}\n",
    "\n",
    "        return tf_idf_score\n",
    "\n",
    "    def check_sent(self, word, sentences):\n",
    "        \"\"\"Check if word is present in sentence list for calculating IDF (Inverse Document Frequency)\"\"\"\n",
    "        final = [all([w in x for w in word]) for x in sentences]\n",
    "        sent_len = [sentences[i] for i in range(0, len(final)) if final[i]]\n",
    "    \n",
    "        return int(len(sent_len))\n",
    "\n",
    "    def get_top_n(self, dict_elem, n):\n",
    "        \"\"\"Calculate most important keywords in text of interest\"\"\"\n",
    "        result = dict(sorted(dict_elem.items(),\n",
    "                     key=itemgetter(1), reverse=True)[:n])\n",
    "        result = result.keys()\n",
    "\n",
    "        return result\n",
    "\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 3. Parse Titles & Articles"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# instantiate News class\n",
    "news = News(news_api_key)\n",
    "# get all news - takes about 30 seconds\n",
    "news.get_all_news()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 4. Get Important Words & Phrases"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# get keywords from article text\n",
    "news.all_news_df[\"keywords\"] = news.all_news_df['text'].apply(news.keyword_extraction)\n",
    "# get top n=3 words of significance\n",
    "news.all_news_df[\"keywords\"] = news.all_news_df[\"keywords\"].apply(\n",
    "     news.get_top_n, n=3)\n",
    "\n",
    "\n",
    "print(news.all_news_df[\"keywords\"])\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "#print(news.all_news_df[\"keywords\"])\n",
    "# a, b, c = news.all_news_df[\"keywords\"].iloc[:,0], news.all_news_df[\"keywords\"][:,2], news.all_news_df[\"keywords\"][:,3]\n",
    "for keys in news.all_news_df[\"keywords\"]:\n",
    "    keywords = list(keys)\n",
    "    a = keywords[0]\n",
    "    b = keywords[1]\n",
    "    c = keywords[2]\n",
    "    print(a, b, c)\n",
    "    #print(keyword[1])\n",
    "    #print(keyword[2])\n",
    "#print(news.all_news_df[\"keywords\"].dict_keys())\n",
    "\n",
    "# a, b, c = news.all_news_df[\"keywords\"][:0], news.all_news_df[\"keywords\"][:1], news.all_news_df[\"keywords\"][:2]\n",
    "\n",
    "# print(a)\n",
    "# print(b)\n",
    "# print(c)\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 5. Search Twitter API\n",
    "## Using Important Words & Phrases"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Create Tweets class"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "source": [
    "class Tweets():\n",
    "\n",
    "    def __init__(self, consumer_key, consumer_secret, access_token, access_token_secret, logger=logging):\n",
    "        self.logger = logging.basicConfig(filename='tweets.log', filemode='w',\n",
    "                                         format=f'%(asctime)s - %(levelname)s - %(message)s')\n",
    "        self.consumer_key = consumer_key\n",
    "        self.consumer_secret = consumer_secret\n",
    "        self.access_token = access_token\n",
    "        self.access_token_secret = access_token_secret\n",
    "\n",
    "    def tweepy_auth(self):\n",
    "        \"\"\"Authorize tweepy API\"\"\"\n",
    "\n",
    "        self.auth = tweepy.OAuthHandler(self.consumer_key, self.consumer_secret)\n",
    "        self.auth.set_access_token(self.access_token, self.access_token_secret)\n",
    "\n",
    "        # create API object\n",
    "        self.api = API(self.auth, wait_on_rate_limit=True)# wait_on_rate_limit_notify=True)\n",
    "\n",
    "        try:\n",
    "            self.api.verify_credentials()\n",
    "            logging.info(\"Tweepy API Authenticated\")\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Error during Tweepy authentication: {e}\")\n",
    "            raise e \n",
    "        return self.api\n",
    "    \n",
    "    def get_tweets(self, news_keywords, news_instance): # TODO add stream listening stuff to params\n",
    "        searched_tweets = self.tweet_search(news_keywords)\n",
    "        stream_tweets = TwitterStreamListener.on_status(listener, tweet_stream)\n",
    "\n",
    "        all_tweets = {}\n",
    "        # process tweets\n",
    "        for tweet in searched_tweets:\n",
    "            # count tweets\n",
    "            pass\n",
    "            # add count to df column?\n",
    "            \n",
    "        for tweet in stream_tweets:\n",
    "            pass\n",
    "        # break tweets apart for table\n",
    "        for tweet in searched_tweets, stream_tweets:\n",
    "            all_tweets[\"tweet_id\"] = tweet['id']\n",
    "\n",
    "            # add all tweets to database! via mogrify\n",
    "\n",
    "            # put tweets in df\n",
    "            self.all_tweets_df = pd.DataFrame.from_dict(all_tweets, columns=[\n",
    "                                                      \"tweet_id\", \"user_id\", \"location\", \"createdAt\", \"tweet_text\"])\n",
    "\n",
    "            self.all_tweets_df.set_index(\"tweet_id\")\n",
    "\n",
    "            # tweets mention count to news df column\n",
    "            news_instance.all_news_df[\"tweet_mention_count\"] = self.all_tweets_df[\"tweet_id\"].apply(\n",
    "                np.count_nonzero)\n",
    "\n",
    "            # clear dataframe?\n",
    "    \n",
    "    def tweet_search(self, news_keywords):\n",
    "        \"\"\"Search for tweets within previous 7 days.\n",
    "            Inputs: \n",
    "                keyword list\n",
    "            Returns: \n",
    "                Tweet list\n",
    "        \"\"\"\n",
    "\n",
    "        for keys in news_keywords:\n",
    "            keywords = list(keys) # TODO add itertools combinations\n",
    "\n",
    "            # collect tweets, filter out retweets\n",
    "            for word in keywords:\n",
    "                tweets = tweepy.Cursor(self.api.search_tweets, q=str(word) + \" -filter:retweets\", lang='en').items()\n",
    "                # print(tweets)\n",
    "                \n",
    "                # tweet_search_dict = {[tweet.id, tweet.user.id, tweet.user.location, tweet.created_at, tweet.text] for tweet in tweets}\n",
    "                # print(tweet_search_dict)\n",
    "        \n",
    "        # self.tweet_search_df = pd.DataFrame.from_dict(tweet_search_dict, columns=[\"tweet_id\", \"user_id\", \"location\", \"createdAt\", \"tweet_text\"])\n",
    "\n",
    "                return tweets\n",
    "        # self.tweet_search_df.set_index(\"tweet_id\")\n",
    "        \n",
    "        #return self.tweet_search_df\n",
    "        #return tweet_search_dict\n",
    "        \n",
    "    def tweet_trends(self):\n",
    "        # returns JSON\n",
    "        self.tweet_trends_list = []\n",
    "\n",
    "        # shows trends available by location\n",
    "        # locations = self.api.trends_available()\n",
    "\n",
    "        # trends by country\n",
    "        self.location = sys.argv[1] # user location as argument variable\n",
    "        self.geo = geocoder.osm(self.location) # object with latitude & longitude of user location\n",
    "        self.closest_trends = self.api.trends_closest(lat=self.geo.lat, long=self.geo.lng)\n",
    "\n",
    "        # trends = tweepy.Cursor(self.api.trends_place(closest_loc[0]))\n",
    "        # trends = tweepy.Cursor(self.api.trends_place(closest_loc[0]['woeid']))\n",
    "\n",
    "        with open(\"twitter_trends.json\", \"w\") as f:\n",
    "            # write results to JSON file\n",
    "            json.dump(self.closest_trends, f)\n",
    "\n",
    "        with open(\"twitter_trends.json\", \"r\") as file:\n",
    "            # create Python object from JSON\n",
    "            twitter_trends_json = file.read().split(\"\\n\")\n",
    "\n",
    "            for tweet in twitter_trends_json:\n",
    "                tweet_obj = json.loads(tweet)\n",
    "\n",
    "                # add info to top_headlines list/dict\n",
    "                self.tweet_trends_list.append(tweet_obj)\n",
    "\n",
    "        # for trend in self.closest_trends:#.items():\n",
    "        #     self.tweet_trends_list.append(trend)\n",
    "            # return self.tweet_trends_list\n",
    "\n",
    "            self.tweet_trends_df = pd.DataFrame(self.tweet_trends_list)\n",
    "            # TODO call .clean_tweets() to put clean data into df\n",
    "\n",
    "        return self.tweet_trends_df \n",
    "    \n",
    "    def clean_tweets(self, tweets):\n",
    "        # use slang.txt\n",
    "        # https://www.geeksforgeeks.org/python-efficient-text-data-cleaning/\n",
    "        pass\n",
    "\n",
    "\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "source": [
    "# keywords = dict(news.all_news_df[\"keywords\"])\n",
    "\n",
    "#print(keywords)\n",
    "t = Tweets(consumer_key, consumer_secret,access_token, access_token_secret)\n",
    "auth = t.tweepy_auth()\n",
    "# search_df = t.tweet_search(keywords)\n",
    "\n",
    "# apply search to keywords\n",
    "# t.tweet_search_df[\"tweet_count\"] = news.all_news_df[\"keywords\"].map(\n",
    "#      t.tweet_search)\n",
    "\n",
    "\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "source": [
    "#print(keywords)\n",
    "t.tweet_search(news.all_news_df[\"keywords\"])\n",
    "#print(type(t.tweet_search_dict))\n"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "<tweepy.cursor.ItemIterator object at 0x7fd2ca4ed940>\n",
      "<tweepy.cursor.ItemIterator object at 0x7fd2cab3d6d0>\n",
      "<tweepy.cursor.ItemIterator object at 0x7fd2ca4ed5e0>\n",
      "<tweepy.cursor.ItemIterator object at 0x7fd2cab3d100>\n",
      "<tweepy.cursor.ItemIterator object at 0x7fd2cab3d2e0>\n",
      "<tweepy.cursor.ItemIterator object at 0x7fd2cab3d700>\n",
      "<tweepy.cursor.ItemIterator object at 0x7fd2cab3d070>\n",
      "<tweepy.cursor.ItemIterator object at 0x7fd2a8f238b0>\n",
      "<tweepy.cursor.ItemIterator object at 0x7fd2a8f23e80>\n",
      "<tweepy.cursor.ItemIterator object at 0x7fd2ca7c9610>\n",
      "<tweepy.cursor.ItemIterator object at 0x7fd2ca4febb0>\n",
      "<tweepy.cursor.ItemIterator object at 0x7fd2ca4fe5e0>\n",
      "<tweepy.cursor.ItemIterator object at 0x7fd2ca4fe790>\n",
      "<tweepy.cursor.ItemIterator object at 0x7fd2cae0c4c0>\n",
      "<tweepy.cursor.ItemIterator object at 0x7fd2cae0ca00>\n",
      "<tweepy.cursor.ItemIterator object at 0x7fd2cae0c9a0>\n",
      "<tweepy.cursor.ItemIterator object at 0x7fd2cae0ccd0>\n",
      "<tweepy.cursor.ItemIterator object at 0x7fd2cae0c070>\n",
      "<tweepy.cursor.ItemIterator object at 0x7fd2cae0cd00>\n",
      "<tweepy.cursor.ItemIterator object at 0x7fd2cae0ce80>\n",
      "<tweepy.cursor.ItemIterator object at 0x7fd2cae0c670>\n",
      "<tweepy.cursor.ItemIterator object at 0x7fd2cae0c4f0>\n",
      "<tweepy.cursor.ItemIterator object at 0x7fd2cae0c580>\n",
      "<tweepy.cursor.ItemIterator object at 0x7fd2caee1670>\n",
      "<tweepy.cursor.ItemIterator object at 0x7fd2caee16a0>\n",
      "<tweepy.cursor.ItemIterator object at 0x7fd2caee1130>\n",
      "<tweepy.cursor.ItemIterator object at 0x7fd2caee1d90>\n",
      "<tweepy.cursor.ItemIterator object at 0x7fd2caee1d60>\n",
      "<tweepy.cursor.ItemIterator object at 0x7fd2caee1640>\n",
      "<tweepy.cursor.ItemIterator object at 0x7fd2caee1d30>\n",
      "<tweepy.cursor.ItemIterator object at 0x7fd2caee14f0>\n",
      "<tweepy.cursor.ItemIterator object at 0x7fd2caee1b50>\n",
      "<tweepy.cursor.ItemIterator object at 0x7fd2caee1df0>\n",
      "<tweepy.cursor.ItemIterator object at 0x7fd2caee1430>\n",
      "<tweepy.cursor.ItemIterator object at 0x7fd2caee1730>\n",
      "<tweepy.cursor.ItemIterator object at 0x7fd2caee12b0>\n",
      "<tweepy.cursor.ItemIterator object at 0x7fd2caee1ca0>\n",
      "<tweepy.cursor.ItemIterator object at 0x7fd2caee1f40>\n",
      "<tweepy.cursor.ItemIterator object at 0x7fd2caee11f0>\n",
      "<tweepy.cursor.ItemIterator object at 0x7fd2caee1e80>\n",
      "<tweepy.cursor.ItemIterator object at 0x7fd2caee1460>\n",
      "<tweepy.cursor.ItemIterator object at 0x7fd2a8f14e20>\n",
      "<tweepy.cursor.ItemIterator object at 0x7fd2cb3eb4f0>\n",
      "<tweepy.cursor.ItemIterator object at 0x7fd2cacba1f0>\n",
      "<tweepy.cursor.ItemIterator object at 0x7fd2cacbafd0>\n",
      "<tweepy.cursor.ItemIterator object at 0x7fd2cacbae50>\n",
      "<tweepy.cursor.ItemIterator object at 0x7fd2c93ac0d0>\n",
      "<tweepy.cursor.ItemIterator object at 0x7fd2a85e4520>\n",
      "<tweepy.cursor.ItemIterator object at 0x7fd2ca3b1bb0>\n",
      "<tweepy.cursor.ItemIterator object at 0x7fd2ca3b19a0>\n",
      "<tweepy.cursor.ItemIterator object at 0x7fd2ca3b1c40>\n",
      "<tweepy.cursor.ItemIterator object at 0x7fd2ca3b1e20>\n",
      "<tweepy.cursor.ItemIterator object at 0x7fd2ca3b1e80>\n",
      "<tweepy.cursor.ItemIterator object at 0x7fd2ca3b1df0>\n",
      "<tweepy.cursor.ItemIterator object at 0x7fd2ca3b1970>\n",
      "<tweepy.cursor.ItemIterator object at 0x7fd2ca3b1b80>\n",
      "<tweepy.cursor.ItemIterator object at 0x7fd2ca3b1be0>\n",
      "<tweepy.cursor.ItemIterator object at 0x7fd2d87d09d0>\n",
      "<tweepy.cursor.ItemIterator object at 0x7fd2a86b79d0>\n",
      "<tweepy.cursor.ItemIterator object at 0x7fd2a86b7c40>\n",
      "<tweepy.cursor.ItemIterator object at 0x7fd2a86b7af0>\n",
      "<tweepy.cursor.ItemIterator object at 0x7fd2a86b7730>\n",
      "<tweepy.cursor.ItemIterator object at 0x7fd2ca4ed7f0>\n",
      "<tweepy.cursor.ItemIterator object at 0x7fd2cae860a0>\n",
      "<tweepy.cursor.ItemIterator object at 0x7fd2ca3b17c0>\n",
      "<tweepy.cursor.ItemIterator object at 0x7fd2ca3b1970>\n",
      "<tweepy.cursor.ItemIterator object at 0x7fd2ca3b1df0>\n",
      "<tweepy.cursor.ItemIterator object at 0x7fd2ca3b1e80>\n",
      "<tweepy.cursor.ItemIterator object at 0x7fd2cae865b0>\n",
      "<tweepy.cursor.ItemIterator object at 0x7fd2ca3b1b20>\n",
      "<tweepy.cursor.ItemIterator object at 0x7fd2ca3b1e50>\n",
      "<tweepy.cursor.ItemIterator object at 0x7fd2ca3b1880>\n",
      "<tweepy.cursor.ItemIterator object at 0x7fd2ca3b1ca0>\n",
      "<tweepy.cursor.ItemIterator object at 0x7fd2ca3b1e20>\n",
      "<tweepy.cursor.ItemIterator object at 0x7fd2ca3b1c40>\n",
      "<tweepy.cursor.ItemIterator object at 0x7fd2ca3b1cd0>\n",
      "<tweepy.cursor.ItemIterator object at 0x7fd2a85e40d0>\n",
      "<tweepy.cursor.ItemIterator object at 0x7fd2a85e45e0>\n",
      "<tweepy.cursor.ItemIterator object at 0x7fd2cacbadf0>\n",
      "<tweepy.cursor.ItemIterator object at 0x7fd2cacba3a0>\n",
      "<tweepy.cursor.ItemIterator object at 0x7fd2cacba5b0>\n",
      "<tweepy.cursor.ItemIterator object at 0x7fd2cacbaa60>\n",
      "<tweepy.cursor.ItemIterator object at 0x7fd2cb3ebb80>\n",
      "<tweepy.cursor.ItemIterator object at 0x7fd2a8f14d90>\n",
      "<tweepy.cursor.ItemIterator object at 0x7fd2caee1f70>\n",
      "<tweepy.cursor.ItemIterator object at 0x7fd2caee17c0>\n",
      "<tweepy.cursor.ItemIterator object at 0x7fd2caee13d0>\n",
      "<tweepy.cursor.ItemIterator object at 0x7fd2caee1970>\n",
      "<tweepy.cursor.ItemIterator object at 0x7fd2caee1280>\n",
      "<tweepy.cursor.ItemIterator object at 0x7fd2caee1820>\n",
      "<tweepy.cursor.ItemIterator object at 0x7fd2caee15b0>\n",
      "<tweepy.cursor.ItemIterator object at 0x7fd2caee1eb0>\n",
      "<tweepy.cursor.ItemIterator object at 0x7fd2caee15e0>\n",
      "<tweepy.cursor.ItemIterator object at 0x7fd2caee1250>\n",
      "<tweepy.cursor.ItemIterator object at 0x7fd2caee1dc0>\n",
      "<tweepy.cursor.ItemIterator object at 0x7fd2cb3eb430>\n",
      "<tweepy.cursor.ItemIterator object at 0x7fd2cacba5b0>\n",
      "<tweepy.cursor.ItemIterator object at 0x7fd2cacba3a0>\n",
      "<tweepy.cursor.ItemIterator object at 0x7fd2cacbadf0>\n",
      "<tweepy.cursor.ItemIterator object at 0x7fd2c93ac0d0>\n",
      "<tweepy.cursor.ItemIterator object at 0x7fd2a85e40d0>\n",
      "<tweepy.cursor.ItemIterator object at 0x7fd2ca3b1af0>\n",
      "<tweepy.cursor.ItemIterator object at 0x7fd2ca3b1d90>\n",
      "<tweepy.cursor.ItemIterator object at 0x7fd2ca3b17f0>\n",
      "<tweepy.cursor.ItemIterator object at 0x7fd2ca3b1910>\n",
      "<tweepy.cursor.ItemIterator object at 0x7fd2ca3b1d60>\n",
      "<tweepy.cursor.ItemIterator object at 0x7fd2ca3b1a60>\n",
      "<tweepy.cursor.ItemIterator object at 0x7fd2ca3b1b50>\n",
      "<tweepy.cursor.ItemIterator object at 0x7fd2ca3b1dc0>\n",
      "<tweepy.cursor.ItemIterator object at 0x7fd2cae86a60>\n",
      "<tweepy.cursor.ItemIterator object at 0x7fd2cae860a0>\n",
      "<tweepy.cursor.ItemIterator object at 0x7fd2caee1550>\n",
      "<tweepy.cursor.ItemIterator object at 0x7fd2caee1e50>\n",
      "<tweepy.cursor.ItemIterator object at 0x7fd2caee1070>\n",
      "<tweepy.cursor.ItemIterator object at 0x7fd2caee1c10>\n",
      "<tweepy.cursor.ItemIterator object at 0x7fd2caee19a0>\n",
      "<tweepy.cursor.ItemIterator object at 0x7fd2caee1d00>\n",
      "<tweepy.cursor.ItemIterator object at 0x7fd2caee1190>\n",
      "<tweepy.cursor.ItemIterator object at 0x7fd2caee1b80>\n",
      "<tweepy.cursor.ItemIterator object at 0x7fd2caee1220>\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# define stream listener class\n",
    "class TwitterStreamListener(tweepy.StreamListener):\n",
    "    def __init__(self, api=None):\n",
    "        super(TwitterStreamListener, self).__init__()\n",
    "        # super(json.JSONEncoder, self).__init__()\n",
    "        self.num_tweets = 0\n",
    "        self.file = open('tweets.txt', 'w')\n",
    "        self.tweet_list = []\n",
    "    \n",
    "    def toJson(self):\n",
    "        return json.dumps(self, default=lambda o: o.__dict__)\n",
    "\n",
    "    def on_status(self, status): # returns JSON \n",
    "  \n",
    "        # Retweet count\n",
    "        # retweet_count = status['retweet_count']\n",
    "        status = status.toJson()\n",
    "        status = json.loads(status)\n",
    "\n",
    "        while self.num_tweets < 450:  # max stream rate is for the twitter API Client\n",
    "            with open('tweets.json', 'w') as f:\n",
    "            # write results to JSON file\n",
    "                json.dump(status, f)\n",
    "\n",
    "            with open('tweets.json', 'r') as file:\n",
    "                # create Python list object from JSON\n",
    "                tweets_json = file.read().split(\"\\n\")\n",
    "\n",
    "                for tweet in tweets_json:\n",
    "                    tweet_obj = json.loads(tweet)\n",
    "\n",
    "                    # Tweet ID\n",
    "                    tweet_obj['tweet_id'] = tweet_obj['id']\n",
    "                    # User ID\n",
    "                    tweet_obj['user_id'] = tweet_obj['user']['id']\n",
    "                    # Username\n",
    "                    tweet_obj['username'] = tweet_obj['user']['name']\n",
    "                    # creation date\n",
    "                    tweet_obj['create_time'] = tweet_obj['user']['creation_date']\n",
    "                    # Language\n",
    "                    lang = status['lang']\n",
    "\n",
    "                    # Tweet\n",
    "                    if status.truncated == True:\n",
    "                        tweet = tweet_obj['extended_tweet']['full_text']\n",
    "                        hashtags = tweet_obj['extended_tweet']['entities']['hashtags']\n",
    "                    else:\n",
    "                        tweet = status.text\n",
    "                        hashtags = status.entities['hashtags']\n",
    "\n",
    "                    # Read hastags using helper function\n",
    "                    hashtags = read_hashtags(hashtags)\n",
    "\n",
    "                    # add info to pop_news dict\n",
    "                    # If tweet is not a retweet and tweet is in English\n",
    "                    if not hasattr(status, \"retweeted_status\") and lang == \"en\":\n",
    "                        self.tweet_list.append(tweet_obj)\n",
    "                \n",
    "                    #self.tweet_list.append(status)\n",
    "                        self.num_tweets += 1\n",
    "            \n",
    "            # flatten data to dataframe\n",
    "            # tweets = pd.json_normalize(self.tweet_list, record_path=['articles'])\n",
    "        #self.tweets_df = pd.DataFrame(self.tweet_list, columns=[\n",
    "                                      #\"tweet_id\", \"publishedAt\", \"userID\", \"text\", \"location\"])\n",
    "\n",
    "        #return self.tweets_df\n",
    "        return self.tweet_list\n",
    "    \n",
    "    # Extract hashtags\n",
    "    def read_hashtags(self, tag_list):\n",
    "        hashtags = []\n",
    "\n",
    "        for tag in tag_list:\n",
    "            hashtags.append(tag['text'])\n",
    "\n",
    "        return hashtags\n",
    "    \n",
    "    \n",
    "    def clean_tweets(self):\n",
    "\n",
    "        with open(\"tweets.json\", \"w\") as f:\n",
    "            # write tweets to json file\n",
    "            json.dump(tweet, f)\n",
    "\n",
    "        with open(\"tweets.json\", \"r\") as file:\n",
    "            # create python object from json\n",
    "            tweets_json = file.read().split(\"\\n\")\n",
    "\n",
    "            for tweet in tweets_json:\n",
    "                tweet_obj = json.loads(tweet)\n",
    "\n",
    "                #flatten nested fields\n",
    "                if 'quoted_status' in tweet_obj:\n",
    "                    tweet_obj['quote_tweet'] = tweet_obj['quoted_status']['extended_tweet']['full_text']\n",
    "                if 'user' in tweet_obj:\n",
    "                    tweet_obj['location'] = tweet_obj['user']['location']\n",
    "                # if 'created_at' in tweet_obj:\n",
    "                #     tweet_obj['created_at'] = pd.to_datetime(tweet)\n",
    "\n",
    "    def on_error(self, status_code):\n",
    "        if status_code == 420:\n",
    "            return False  # false disconnects the stream\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "\n",
    "auth = t.tweepy_auth()\n",
    "# instantiate Tweet Stream Listener\n",
    "listener = TwitterStreamListener()\n",
    "# authenticate stream\n",
    "tweet_stream = tweepy.Stream(auth, listener, access_token, access_token_secret) #tweet_mode=\"extended\")\n",
    "listener.on_status(tweet_stream)\n",
    "\n",
    "tweepy.get_tweets()\n",
    "# print cleaned tweets df\n",
    "\n",
    "# print(news.all_news_df)\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 6. Search TikTok\n",
    "## Using Important Words & Phrases"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "\"\"\"Search TikTok for videos related to keywords parsed from news articles\"\"\"\n",
    "\n",
    "fp = c['tiktokAuth']['s_v_web_id']\n",
    "keywords = list(news.all_news_df[\"keywords\"])\n",
    "\n",
    "class TikToks(TikTokApi):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(TikTokApi, self).__init__()\n",
    "        self.tiktok_list = []\n",
    "\n",
    "    #@Timer(\"Tiktok Download\")\n",
    "    def get_tiktok_trends(self, keywords):\n",
    "        # returns tiktok dictionary/JSON\n",
    "        self.api = TikTokApi()\n",
    "        self.api.get_instance(custom_verifyFp=fp, use_test_endpoints=True, use_selenium=True)\n",
    "        trends = self.api.by_hashtag(keywords)\n",
    "\n",
    "        with open(\"tiktoks.json\", \"w\") as f:\n",
    "            json.dump(trends, f)\n",
    "\n",
    "        with open(\"tiktoks.json\", \"r\") as file:\n",
    "            toks_json = file.read().split(\"\\n\")\n",
    "\n",
    "            # create json object for each tiktok\n",
    "            for tok in toks_json:\n",
    "                tok_obj = json.loads(tok)\n",
    "                \n",
    "                if 'id' in tok:\n",
    "                    tok_obj['userID'] = tok_obj['author']['id']\n",
    "                    tok_obj['postID'] = tok_obj['id']\n",
    "                if 'signature' in tok:\n",
    "                    tok_obj['user_bio'] = tok_obj['author']['signature']\n",
    "                if 'challenges' in tok:\n",
    "                    # iterate over multiples\n",
    "                    tok_obj['tagID'] = tok_obj['challenges']['id']\n",
    "                    tok_obj['tag_name'] = tok_obj['challenges']['title']\n",
    "                if 'createTime' in tok:\n",
    "                    tok_obj['createTime'] = tok_obj['createTime']\n",
    "                if 'desc' in tok:\n",
    "                    tok_obj['description'] = tok_obj['desc']\n",
    "                if 'stats' in tok:\n",
    "                    tok_obj['comment_count'] = tok_obj['stats']['commentCount']\n",
    "                    tok_obj['digg_count'] = tok_obj['stats']['diggCount']\n",
    "                    tok_obj['play_count'] = tok_obj['stats']['playCount']\n",
    "                    tok_obj['share_count'] = tok_obj['stats']['shareCount']\n",
    "                if 'video' in tok:\n",
    "                    tok_obj['videoID'] = tok_obj['itemList']['video']['id']\n",
    "                if 'sound' in tok:\n",
    "                    tok_obj['soundID'] = tok_obj['sound']['id']\n",
    "                    tok_obj['soundTitle'] = tok_obj['sound']['title']\n",
    "                    tok_obj['isOriginal'] = tok_obj['sound']['original']\n",
    "                if 'music' in tok:\n",
    "                    tok_obj['songID'] = tok_obj['music']['id']\n",
    "                    tok_obj['songTitle'] = tok_obj['music']['title']\n",
    "                \n",
    "                self.tiktok_list.append(tok_obj)\n",
    "\n",
    "            # create tiktok dataframe\n",
    "            self.toks_df = pd.DataFrame(self.tiktok_list)\n",
    "\n",
    "            # split df by columns corresponding to tables\n",
    "\n",
    "        return self.toks_df\n",
    "\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "tiktoks = TikToks()\n",
    "tiktoks.get_tiktok_trends(keywords)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 7. Add Late-Arriving Dimensions/Data\n",
    "### *data corresponding to 3 days before news hit"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# tweet search instead of stream"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 7. Tally Up\n",
    "### Partition total mentions by day"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Add to database"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# mogrify stream\n",
    "postgres_db.execute_mogrify(connection, filtered_stream, 'stream_tweets')\n",
    "# mogrify batch tweets\n",
    "postgres_db.execute_mogrify(connection, batch_tweets, 'batch_tweets')\n",
    "# mogrify trends\n",
    "postgres_db.execute_mogrify(connection, tweet_trends, 'tweet_trends')\n",
    "# execute mogrify - insert news & keywords into database\n",
    "postgres_db.execute_mogrify(connection, news.all_news_df, 'articles')\n",
    "\n",
    "# TODO group by event? "
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 9. Plot & Analyze\n",
    "- On which platform (Twitter or TikTok) do folks engage with politics the most?\n",
    "- Where in the US is engagement the highest?\n",
    "- Which political events seem to cause the most reaction among youth?"
   ],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.9.6",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.9.6 64-bit ('data_eng_projects': conda)"
  },
  "interpreter": {
   "hash": "292a7ef3bed24448555716cf5e78212297c16b0757a55b8597bb1f802fb134d0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}